---
title: "Seminar on DNA Data Storage"
subtitle: "Improved read/write cost tradeoff in DNA-based data storage using LDPC codes<br><br>Shubham Chandak, Kedar Tatwawadi, Billy Lau, Jay Mardia, Matthew Kubit, Joachim Neu, Peter Griffin, Mary Wootters, Tsachy Weissman, Hanlee J<br>(2019)<br><br>Presented by: Atar Ron and Mattan Hoory"
format:
  revealjs: 
    title-slide-attributes:
      data-state: "hide-menubar"
    slide-number: true
    preview-links: auto
    css: style.css
    logo: assets/CS_LOGO.jpg
    footer: 'DNA Data Storage - 02360801 - Spring 2025'
    toc: true
    toc-depth: 1
    simplemenu:
        flat: true
        barhtml:
            header: "<div class='menubar'><ul class='menu'></ul><div>"
        scale: 0.42

revealjs-plugins:
  - simplemenu
---
  
::: {.center}
# 1 Introduction & Context {data-name="Intro"}
:::

## Background

In DNA-based data storage, there are two critical challenges:

::: {.incremental}

- **Write cost** — how much synthetic DNA we need to store one bit of information  
- **Read cost** — how many sequencing reads are needed to reliably recover that bit


<span class= "fragment"> These two costs are closely related, and thus, one of the most important problems in DNA data storage is finding an effective tradeoff between them. </span>

<div style="text-align: center;">
  <img src="assets/write_read_proc.PNG" width="600px" height="300px" class="fragment">
</div>

:::

---

## Discussion Questions

::: {.incremental}

- Why , in your opinion, are the cost of write and cost of read closely related? how does the write cost affect the read cost?
&nbsp;  
- What are the main factors that affect the cost of write?  
&nbsp;  
- What are the main factors that affect the cost of read?

:::


---

## Main Goals

::: {.incremental}

Since the cost of write and cost of read are strongly correlated, this paper aims to:


- Establish a theoretical lower bound on the tradeoff between write cost and read cost.
- Design and evaluate a practical coding scheme (based on LDPC codes) that achieves a better tradeoff than previous methods.
- Validate the performance of the scheme through both real experiments (DNA synthesis and sequencing) and simulations.

:::

## Cost of Read and Write 

* **Cost Of Write**
  Average number of encoded bits synthesized per information bit:

  $$
  c_w = \frac{|\text{SYN}|}{|\text{DATA}|}
  $$

* **Cost of Read**
  Average number of bits read per information bit:

  $$
  c_r = \frac{|\text{READ}|}{|\text{DATA}|}
  $$

## Coverage

* **Coverage**- Average number of bits read per synthesized bit:

  $$
  \text{Coverage} = \frac{|\text{READ}|}{|\text{SYN}|}
  = \frac{\frac{|\text{READ}|}{|\text{DATA}|}}{\frac{|\text{SYN}|}{|\text{DATA}|}}
  = \frac{c_r}{c_w}
  $$


::: {.incremental}
- Coverage has been widely used in prior work to estimate the efficiency of read/write tradeoffs in DNA-based storage systems. 
<br> </span>

- It measures how many sequencing reads are made per synthesized bit — but is that the best way to evaluate system performance?
<br></span>

:::
---

## Is Coverage a Good Metric?

<span style="font-size:0.9em">We consider the following example that demonstrates why coverage can be misleading.</span><br>

### Example
<span style="font-size:0.9em">Suppose we compare two storage systems with the following properties:</span>

| System        | $c_w$ | $c_r$ | $\text{coverage} = c_r/c_w$ |
|----------------|------------|----------------|------------------------|
| A  | 4                   | 12                 | 3                   |
| B  | 2                   | 10                 | 5                   |

* <span style="font-size:0.9em">**Note:** In this example, we assume that both the read cost and the coverage values were measured after decoding all the sampled strands.</span>



## Example - cont. 

| System        | $c_w$ | $c_r$ | $\text{coverage} = c_r/c_w$ |
|----------------|------------|----------------|------------------------|
| A  | 4                   | 12                 | 3                   |
| B  | 2                   | 10                 | 5                   |

* <span style="font-size:0.9em">At first glance, System A has better (lower) coverage. Therefore, when evaluating the systems based on coverage only, we will prefer System A. </span>  


* <span style="font-size:0.9em"> But, it is easy to see that System B reads fewer total bits per information bit (10 vs. 12), and also synthisizes significantly less DNA. So despite having higher coverage, System B is clearly more efficient overall. </span>


---

## Example - cont. 
<br>
**Conclusion:** <span style="font-size:0.9em">This example illustrates that relying solely on coverage can be misleading, particularly when comparing systems with varying write costs. A more meaningful comparison is to evaluate the actual read and write costs per information bit.</span>  
<br>

**Important Note:** <span style="font-size:0.9em">We can also define coverage at the strand level: the average number of times a strand is observed in the sampled reads. We'll refer back to this concept later on.</span>


---

## Model Notations

- $n$ – Number of strands.  
- $L$ – Strand length.  
- $c_{w}$ – Cost of write.  
- $c_{r}$ – Cost of read.  
- $|\text{SYN}|$ – Total number of bases synthesized.  
- $|\text{READ}|$ – Total number of bases read.
- $\epsilon$ - Substitution error rate.


## Model Definition {.nostretch}

::: {.fragment data-fragment-index="1"}
- <span style="font-size:0.8em;color:#00B0F0" >The storage system encodes $n$ information strands, each of length $L$. </span>
:::

::: {.fragment data-fragment-index="3"}
- <span style="font-size:0.8em;color:#059B2C">To store the data with write cost $c_w$, the encoder synthesizes $n \cdot c_w$ strands of length $L$.</span>
:::

::: {.fragment data-fragment-index="5"}
- <span style="font-size:0.8em;color:#059B2C"> To retrieve the data with read cost $c_r$, the decoder samples $n \cdot c_r$ strands of length $L$. </span> <br><br>
:::

<!-- Images side-by-side and controlled with fragments -->
<div style="margin-bottom: 200px;">
  <img src="assets/model_process1.PNG" width="200" class="fragment" data-fragment-index="2">
  <img src="assets/model_process2.PNG" width="350" class="fragment" data-fragment-index="4">
  <img src="assets/model_process3.PNG" width="360" class="fragment" data-fragment-index="6">
</div>


---


## Model Definition - cont.

<!-- Final bullet -->
- <span style="font-size:0.7em"> It is assumed, for simplicity, that the decoder has access to the index of each strand, and that deletion and insertion errors are ignored. We later explain how the authors overcame this in practice. </span>

- <span style="font-size:0.7em"> The reads are subject to:</span>

  - <span style="font-size:0.7em"> **Substitution errors** — each sampled strand is passed through a Binary Symmetric Channel (BSC) with bit-flip probability $\epsilon$. This simulates sequencing errors.</span>
  - <span style="font-size:0.7em"> **Sampling variability** — the number of times each strand is sampled is random (Poisson-distributed).</span>

![](assets/model_process4.PNG){width=900px height=180px fig-align="center"}


---

## Data Distribution

- <span style="font-size:0.7em">For the sake of the theoretical analysis, it is assumed that the sampled DNA strands follow a Poisson distribution.</span>
- <span style="font-size:0.7em">Although the standard Poisson model does not account for synthesis or sequencing biases, it still provides significant insights into strand sampling behavior.</span>

#### Poisson Recap:
- <span style="font-size:0.7em">The Poisson distribution models the number of times an event occurs in a fixed interval, given a known average rate $\lambda$.</span>
- <span style="font-size:0.7em">The probability mass function is given by:</span>

:::{.small_math}  
$$
P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}
$$

:::

## Why Poisson?

::: {.incremental}

- <span style="font-size:0.7em">In the paper, the authors assume that each of the $n \cdot c_r$ sequenced strands is sampled independently and uniformly at random from the $n \cdot c_w$ synthesized strands.

- <span style="font-size:0.7em">This defines a **multinomial distribution** over the $n \cdot c_w$ strands (trials), where each trial has a selection probability of $\frac{1}{n \cdot c_w}$ per draw.

- <span style="font-size:0.7em">When the number of strands is large then the individual selection probability of each strand is small.

- <span style="font-size:0.7em"> Therefore, we can apply the **Poisson approximation to the multinomial**:</span>
  <span style="font-size:0.7em">Each strand is sampled independently according to a $\text{Poisson}(\lambda)$ distribution, where:


:::{.fragment .small_math}
$$
\lambda = \text{#Samples}*\text{Proability}= \frac{nc_r}{nc_w} = \frac{c_r}{c_w}
$$
:::

:::


::: {.center}
# 2 Communication and Information 101 {data-name="Theory"}
:::

---

## Communication and Information - Motivation

- We’ve seen that the **write cost** $c_w$ and **read cost** $c_r$ are tightly connected. With an optimal code, increasing $c_w$ typically reduces $c_r$. <br><br>

- However, a higher write cost means more redundancy, which **lowers the information rate** per bit. <br><br>

- In this section, we introduce basic **channel theory** to better understand and analyze the **tradeoff between $c_w$ and $c_r$**.


::: {.notes}
- We have shown that the cost of read and cost of write are closely related, and that the coverage is not a good metric to evaluate the tradeoff between them.
- To understand how, we need to understand the theoretical foundations of communication and information theory.
:::
---

## What is a Channel?

A **channel** is a mathematical model used in information theory to describe how information is transmitted from a sender to a receiver.

- Input $X$: a message of n bits.
- Output $Y$: a possibly altered version of the message

The channel introduce **noise**, leading to errors or loss.

![](assets/channel.PNG){width=800px height=300px fig-align="center"}

---

## Example: Binary Erasure Channel (BEC) 
- <span style="font-size:0.8em">BEC is a channel where each transmitted bit is erased with probability $\varepsilon$.

- <span style="font-size:0.8em">Input: $n$ bits from $\{0, 1\}$.</span>
- <span style="font-size:0.8em">Output: $n$ bits from $\{0, 1,?\}$, where $?$ denotes an erasure.</span>

<span style="font-size:0.8em">Each bit is either received correctly or erased with probability $\varepsilon$.</span>

![](assets/BEC_example.PNG){width=300px height=300px fig-align="center"}


## Entropy

- <span style="font-size:0.8em">**Entropy** quantifies the uncertainty or randomness in a random variable. </span>
- <span style="font-size:0.8em">Formally, if a discrete random variable $X$ takes values $x_1, \dots, x_n$ with probabilities $p_1, \dots, p_n$ , then:</span>

$$
H(X) = -\sum_{i=1}^n p_i \log_2 p_i
$$

- <span style="font-size:0.8em">The entropy is always between 0 and 1: $$0 \leq H(X) \leq 1$$</span>
- <span style="font-size:0.8em">Higher entropy means more uncertainty.</span>
- <span style="font-size:0.8em">Lower entropy means the variable is more predictable.</span>

---

## Entropy – Cont.

- $H(X) = 0$ when $X$ is completely predictable (e.g., a constant value).
- $H(X) = 1$ when $X$ is completley uncertain. This occurs when $X \sim \text{Bern}(\tfrac{1}{2})$.
- Entropy satisfies a version of the law of total probability:

$$
H(X) = \sum_y P(Y = y) \cdot H(X \mid Y = y)
$$

![](assets/entropy.PNG){width=600px height=300px fig-align="center"}

---

## Capacity of a Binary Erasure Channel (BEC)

<span style="font-size:0.7em"> Assume a bit $X\sim\mathrm{Bern}\bigl(\tfrac12\bigr)$ and a Binary Erasure Channel with erasure probability $\varepsilon$. Let $Y$ be the output of the channel. Then:</span>

- <span style="font-size:0.7em"> The entropy of $X$ holds: $H(X)=1$</span>
- <span style="font-size:0.7em"> The conditional entropy of $X$ given $Y$ is:</span>

::: {.small-math}
$$  
\begin{aligned}  
H(X \mid Y)
&= P(Y = ?) \cdot H(X \mid Y = ?) + P(Y \neq ?) \cdot H(X \mid Y \neq ?) \\
&= \varepsilon \cdot 1 + (1 - \varepsilon) \cdot 0 \\
&= \varepsilon
\end{aligned}
$$

:::

- <span style="font-size:0.7em">This holds because when the output is "?"", the input is completely uncertain, i.e., $P(X=1) = P(X=0) = 0.5$. In contrast, when the output is 0 or 1, the input is known with certainty, as defined by the Binary Erasure Channel (BEC).</span>


---

## Capacity of a Binary Erasure Channel (BEC)
**<span style="font-size:1.1em">Mutual information </span>** 

- Mutual information is a measure of how much information the output $Y$ gives on the input $X$.

- It is defined as:
   $$  
   \begin{aligned}  
   I(X;Y)
   &=H(X)\;-\;H(X\mid Y)\\  
   \end{aligned}
   $$

  - In our case: 

    $$  
   \begin{aligned}  
   I(X;Y)
   &=H(X)\;-\;H(X\mid Y)=\\
   &=1\;-\;\varepsilon  
   \end{aligned}
   $$

---

## Capacity of a Binary Erasure Channel (BEC)
**<span style="font-size:1.1em">Capacity </span>**  

- Capacity is the maximum reliable information rate of a channel.
- It tells us what is the maximum number of bits per channel use that can be transmitted reliably.
- The capacity is defined as:
$$
C = \max_{p(x)} I(X; Y)
$$

---

## Capacity of a Binary Erasure Channel (BEC) {.smaller}
**<span style="font-size:1.1em">Capacity - cont.</span>**  

- The maximum mutual information over all input distributions defines the channel capacity—that is, the highest rate at which information can be reliably transmitted.

- No other input distribution can achieve a higher reliable rate, as this value is by definition maximal.

- In the case of the Binary Erasure Channel (BEC), the mutual information $I(X;Y)$ is maximized when $X$ is uniformly distributed, i.e., $P(X=0) = P(X=1) = 0.5$.

- Under this distribution, we get $I(X;Y) = 1 - \varepsilon$, and thus the capacity of the channel is $C = 1 - \varepsilon$.


---


## Code Rate

- In coding theory, the **code rate** $R$ quantifies the rate of **information** transmitted per **encoded bit**.

- Recall that the write cost $c_w$ is defined as:  
  $$
  c_w = \frac{|\text{SYN}|}{|\text{DATA}|}
  $$
  which implies:
  $$
  \frac{1}{c_w} = \frac{|\text{DATA}|}{|\text{SYN}|} = R
  $$

- In other words, the **inverse write cost** captures how much information is packed into each encoded bit, which is the code rate.


---

## Why Must the Rate Be ≤ Capacity? {.smaller}

- Recall that the **channel capacity** $C$ represents the **maximum number of information bits per symbol** that can be decoded reliably.

- The **code rate** $R$ reflects the **amount of information per channel symbol** that is being transmitted.

- If $R > C$, then the encoder is injecting **more information** per symbol than the channel can reliably preserve — meaning **some information must be lost** or **decoded incorrectly**.

- Therefore, to achieve **reliable communication**, it is necessary that $R\leq C$.

- According to Shannon's theorem, this is a fundamental limit of communication systems, and according to the Noisy-Channel Coding Theorem, it is achievable with the right coding scheme.

---

## Numerical Example
Assume we have a BEC with $\varepsilon=0.6$ and an encoder with $c_w=2$.

- $R=\frac{1}{c_w}=\frac{1}{2}$.
- $C=1-\varepsilon = 1-0.6=0.4$
- It holds that $R=0.5>0.4$
- Therefore, in this model, the data might not be decoded correctly, or might miss some information.

---

## Binary Symmetric Channel (BSC)
- A Binary Symmetric Channel (BSC) is a channel where each bit is flipped with probability $\varepsilon$.
- The difference between BSC and BEC is that in BSC, the output is always a bit (0 or 1), while in BEC, the output can also be an erasure symbol (?). 
- We do not know where the substitution occurred, but we know that it did occur.

![](assets/BSC.PNG){width=400px height=270px fig-align="center"}

## Recap
- The storage model discussed so far is composed on $nc_w$ strands that are synthesized, and $nc_r$ strands that are sampled from a poisson distributed data. The index of each strand remains intact.

- The data is transmited through a Binary Symmetric Channel (BSC) with substitution rate $\varepsilon$.

- In a Binary Erasure Channel (BEC), the capacity of the channel must be greater or equal to the rate of the code. 

- The capacity of a BEC must be greater or equal to the code rate.

- We now proceed to analyze the lower bounds of the trdeoff between $c_w$ and $c_r$ with different values of $\epsilon$.


    
::: {.center}
# 3 Theoretical bounds {data-name="Bounds"}
:::

## The Case $\varepsilon = 0$

- When $\varepsilon = 0$, there are **no sequencing errors**, so each read is error-free.

- Each DNA strand is sampled independently, and the number of times a strand appears follows a **Poisson distribution** with mean $\lambda = \frac{c_r}{c_w}$

- The probability that a given strand is not observed at all (i.e., the Poisson variable equals 0) is:  $\mathbb{P}[\text{strand unseen}] = e^{-\lambda}$

- If a strand is not observed in the read process, we effectively have an **erasure**.

---

## The Case $\varepsilon = 0$ - cont.

- Thus, the channel can be modeled as a **Binary Erasure Channel (BEC)** where the erasure probability is $\varepsilon = e^{-c_r / c_w}$.

- Since the capacity of the BEC must be greater or equal to the rate of the code, we get that: 

$$
R=\frac{1}{c_w}\leq 1-e^{-\frac{c_r}{c_w}}= C
$$

Simplifying the equation, we get the following lower bound for the cost of read:

$$
c_r\geq c_w ln(\frac{c_w}{c_w-1})
$$

- We can see that as $c_w$ increases, $c_r$ decresed, and vice-versa. This fits our intuition. 


## The case $\epsilon \neq 0$ 



- Now, each bit that is sampled is transmitted through a BSC with error probability $\varepsilon>0$.

- Recall that it is assumed that the index of each strand remains intact, and that each strand is sampled $k$ times, where $k$ is $Poisson(\frac{c_r}{c_w})$ distributed.

- Assume that for each bit in each strand, we are given a tuple ($k_0,k_1$) that indicates how how many times the bit was sampled as 0 and 1, consecutively. Note that $k_0+k_1=k$.

- The probability that the samples of the bit are ($k_0,k_1$) given that the bit has value 0 is:

---

## The case $\epsilon \neq 0$ - cont.

$$
P((k_0,k_1)|0)=\frac{e^{-\lambda}\lambda^{k_0+k_1}}{(k_0+k_1)!} {k_0+k_1\choose k_1} (1-\varepsilon)^{k_0}\varepsilon^{k_1}
$$

* The term $\frac{e^{-\lambda}\lambda^{k_0+k_1}}{(k_0+k_1)!}$ is the probability that a poisson random variable gets the value $k_0+k_1$. 

* the term ${k_0+k_1\choose k_1} (1-\varepsilon)^{k_0}\varepsilon^{k_1}$ is the binomial probability that there are $k_1$ errors out of $k_0+k_1$ samples. 

* The probability of observing the sample counts $(k_0, k_1)$ given that the bit is 1 is symmetric with respect to swapping $k_0$ and $k_1$.

* The following example illustrates how the calculations are performed.

---

## The case $\epsilon \neq 0$ - Example

- In this example, we assume that \epsilon=0.1$ and $\lambda=3$.


::: {.center}

<table style="font-family: monospace; font-size: 1.3em; border-collapse: separate; border-spacing: 0 6px;">
  <tr style="border-bottom: 2px solid #999;">
    <th style="padding: 0px 12px;">Source</th>
    <th style="padding: 0px 12px;">Bit 1</th>
    <th style="padding: 0px 12px;">Bit 2</th>
    <th style="padding: 0px 12px;">Bit 3</th>
    <th style="padding: 0px 12px;">Bit 4</th>
  </tr>
  <tr>
    <td style="padding: 0px 12px;"><strong>Original</strong></td>
    <td style="color: black;">1</td>
    <td style="color: black;">0</td>
    <td style="color: black;">1</td>
    <td style="color: black;">0</td>
  </tr>
  <tr>
    <td style="padding: 0px 12px;">Sample 1</td>
    <td style="color: blue;">1</td>
    <td style="color: red;">0</td>
    <td style="color: blue;">1</td>
    <td style="color: red;">0</td>
  </tr>
  <tr>
    <td style="padding: 0px 12px;">Sample 2</td>
    <td style="color: blue;">1</td>
    <td style="color: blue;">1</td>
    <td style="color: blue;">1</td>
    <td style="color: red;">0</td>
  </tr>
  <tr>
    <td style="padding: 0px 12px;">Sample 3</td>
    <td style="color: red;">0</td>
    <td style="color: red;">0</td>
    <td style="color: blue;">1</td>
    <td style="color: red;">0</td>
  </tr>
  <tr>
    <td style="padding: 0px 12px;">$(k_0,k_1)$</td>
    <td>(1,2)</td>
    <td>(2,1)</td>
    <td>(0,3)</td>
    <td>(3,0)</td>
  </tr>
  <tr>
    <td style="padding: 0px 12px;">Probability</td>
    <td>0.054</td>
    <td>0.054</td>
    <td>0.163</td>
    <td>0.163</td>
  </tr>
</table>




:::



---

## The case $\epsilon \neq 0$ - cont.

- The authors adopt a per-bit perspective to analyze the lower bound on performance.

- They use the bit-level probability in simulations to empirically estimate a lower bound on the read cost, $c_r$.

- Since the analysis is performed at the bit level rather than the strand level, and the bound is based on simulation results, it is important to note that the bound is empirical and may not be tight.


## $c_r$ vs $c_w$: Theoretical bounds 

- This graph demonstrates the lower bounds of the cases $\varepsilon=0$ and $\varepsilon=0.5%$.

- The authors chose $\varepsilon=0.5%$ because this is the substitution probability of illumina sequencing.

![](assets/Graph_1.PNG){width=550px height=450px fig-align="center"}


::: {.center}
# 4 Comparison of coding strategies {data-name="Encoding Strategies"}
:::

---

## Coding Strategies - Introduction

When designing an encoding and decoding scheme, we must account for **two primary sources of error**:

- **Outer Code Errors** — Since the data is divided into multiple strands, some strands may never be sampled or sequenced. The outer code must handle such **erasure errors** and enable recovery of the missing data.

- **Inner Code Errors** — During synthesis and sequencing, strands may undergo **substitution**, **insertion**, or **deletion** errors. The inner code must correct these errors to recover the original encoded information.


---

## Coding Strategies

- In the DNA-based storage world, there is a variety of coding schemes and techniques to recover inner and outer code errors.

- The authors of the paper consider 2 coding strategies:
  * **Inner/Outer Code Separation** - a coding startegy that is composed of an inner code, which handles error inside strands, and outer code, which handles errors in the order of the strands. 
  * **Single Large Block Code** - a coding strategy where large block of inforamtion us encoded, and then it is segmented into strands. 

- Most of works on DNA-based storage prior to this work used the seperation technique, some incorporating elements from the single large block strategy.


## Inner/Outer Code Strategy

<!-- TODO (MATTAN) add a better visualisation. -->

::: {.incremental}

- <span style="font-size:0.8em">This strategy uses two layers of coding:

  - <span style="font-size:0.8em">**Outer code**: Applied across information segments to correct **erasures**.
  - <span style="font-size:0.8em">**Inner code**: Applied within each strand to correct **substitution errors**.

- <span style="font-size:0.8em">The decoder groups reads by index and applies **majority voting** per position. Then:
  - <span style="font-size:0.8em">The **inner decoder** corrects strand-level errors.
  - <span style="font-size:0.8em">The **outer decoder** recovers missing segments.

- <span style="font-size:0.8em">This approach is less effective for short blocks, but near-optimal erasure codes exist for large $n$.

:::


![](assets/inner_outer.PNG){width=900px height=180px fig-align="center"}



## Single large block code

::: {.incremental}

- <span style="font-size:0.8em">Instead of using two separate codes, this approach encodes the **entire data block** using one large **LDPC (Low-Density Parity-Check) code**, and then seperates the encoded block into segments. </span>

- <span style="font-size:0.8em">Each segment contains a small portion of a single, long LDPC codeword. </span>

- <span style="font-size:0.8em">During decoding:</span>
  - <span style="font-size:0.8em">All sequenced strands are aligned using their **indices**.</span>
  - <span style="font-size:0.8em">The collected data is treated as a **noisy, partial version** of the LDPC codeword.</span>
  - <span style="font-size:0.8em">A **belief propagation** algorithm is used to recover the full data block.</span>

- <span style="font-size:0.8em">This method enables a better read/write cost tradeoff.</span>

:::
 

![](assets/large_block.PNG){width=600px height=190px fig-align="center"}


---

## Coding Strategies - Summary

- The following table summarizes the role of each type of code in handling **erasure** and **substitution** errors:

<br>

::: center
| Code Type        | Erasure Errors | Substitution Errors |
|------------------|----------------|----------------------|
| Inner Code       | ✗              | ✓                    |
| Outer Code       | ✓              | ✗                    |
| Large Block Code | ✓              | ✓                    |
:::

---

## $c_r$ vs $c_w$: Simulation Bounds (Separated vs. Large Block)

- <span style="font-size:0.8em">The graph shows the performance of both coding strategies—**separated blocks** and a **single large block**—compared to the theoretical bound. </span>

- <span style="font-size:0.8em">The simulation uses the parameters: $n = 1000$, $L = 256$. </span>


- <span style="font-size:0.8em">The authors used a **BCH code** as the inner code and a **Raptor code** as the outer code—both known to perform well under the chosen settings. </span>

![](assets/Graph_2.PNG){width=700px height=400px fig-align="center"}



::: {.center}
# 5 Low Density Parity Check (LDPC) {data-name="LDPC"}
:::
## LDPC Codes

- LDPC (Low-Density Parity-Check) codes are a powerful class of linear error-correcting codes.
- First introduced by Gallager in the 1960s, but gained widespread adoption in the 1990s with improved decoding algorithms.
- They are now used in modern systems such as Wi-Fi, 5G, satellite communication, flash storage, and digital broadcasting.
- LDPC codes achieve performance close to the Shannon limit, making them highly efficient for reliable communication.
- They support fast and scalable decoding using iterative message-passing algorithms.


## LDPC Motivation
Assume we have the following data to be transmitted:

::: {.big-math}
$$
1\hspace{1mm}0\hspace{1mm}0\hspace{1mm}1\hspace{1mm}1\hspace{1mm}0
$$
:::

and after we transimetted the message, it was recieved as:

::: {.big-math}
$$
1\hspace{1mm}?\hspace{1mm}0\hspace{1mm}1\hspace{1mm}1\hspace{1mm}0
$$
:::

- What can help us recover the erasured bit?

---

## Pairity Check
We can enhance our data by adding a parity check bit:

<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 0 0 1 1 0 <span class="fragment" style="color:red;">1</span>
</div>

<br><br>

<span class="fragment">
Suppose we later receive the following (with one bit erased):
<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 ? 0 1 1 0 <span style="color:red;">1</span>
</div>
</span>

<br>

<span class="fragment">
To recover the missing bit, we compute the parity of the known bits and compare it to the parity check bit. This allows us to deduce the value of the erased bit.
</span>


---

## What happens if more than 1 erasure?

Assume with have the same data as before with the pairity check bit:

<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 0 0 1 1 0 <span style="color:red;">1</span>
</div> <br>

And now, after transmitting this data, the data recieved is:<br><br>
<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 ? 0 1 ? 0 <span style="color:red;">1</span>
</div> <br>
Can our pairity check bit help us recover both erasured bits?

---

## Naive Idea

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.

![](assets/naive1.PNG){width=600px height=290px fig-align="center"}

---

## Naive Idea

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.


![](assets/naive2.PNG){width=600px height=290px fig-align="center"}

---

## Naive Idea

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.

![](assets/naive3.PNG){width=600px height=290px fig-align="center"}

---

## Naive Idea

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.

![](assets/naive4.PNG){width=600px height=290px fig-align="center"}

---

## Naive Idea

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.

![](assets/naive5.PNG){width=600px height=290px fig-align="center"}

<span class="fragment"> So now, when the same erasures as before will occur:
<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 ? 0 1 ? 0 <span style="color:red;">1 0</span>
</div><br>
We will be able to recover the erasured bits.
<br><br>
</span>
<span class="fragment">
Are we done? 
</span>


---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa1.PNG){width=900px height=400px}





---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa2.PNG){width=900px height=400px}


## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa3.PNG){width=900px height=400px}

---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa4.PNG){width=900px height=400px}

---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa5.PNG){width=900px height=400px}

---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa6.PNG){width=900px height=400px}

---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa7.PNG){width=900px height=400px}

---

## Possible Solution  
### Overlapping Parity Checks

- <span style="font-size:0.8em">In the overlapping parity check example, we showed how to correct any 2 erasures in a 4-bit message. </span>

- <span style="font-size:0.8em">What would happen if we apply this strategy repeatedly over consecutive 4-bit blocks in a longer message?</span>

![](assets/4-bit_ex.PNG){width=600px height=150px fig-align="center"}

- <span style="font-size:0.8em">And what if we increase the overlap size together with the amount of pairity checks per overlap — how does that impact performance and error correction?</span>

![](assets/16-bit_ex.PNG){width=800px height=140px fig-align="center"}

---

## Possible Solution
### Parity Check Bits Protection

<!-- TODO continue the intuition with this case -->

<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1  0  ?  1  1  0  <span class="fragment" style="color:red;">?  1  ?</span>
</div>

- Consider our previous example:
![](assets/ldpc_exa7.PNG){width=900px height=400px}

- Assume that the erasures are as follows:




- A natural question arises: how are the **parity check bits** protected, given that they are also subject to substitution errors?

- The solution lies in the **structure of LDPC codes**:  
  Parity bits are **not isolated**—they are also included in other parity check equations.

- By involving **both data and parity bits** in the parity constraints, LDPC codes ensure that **parity bits are themselves protected** and can be recovered during decoding.

- This design enables the **belief propagation algorithm** to iteratively correct errors in both data and parity bits, and effectively recover all the data.


## Density and the Role of Overlap

- Therefore, our goal is to design a parity-based error correction code that:
  - Has **low write cost**,  
  - Enables **fast and efficient decoding**,  
  - **recovers erasures effectively**.

- In the 1960s, Robert Gallager introduced a groundbreaking approach for this challenge — now known as LDPC codes (Low-Density Parity-Check codes).

---

## Density and the Role of Overlap
- Gallager showed how to **balance two competing goals**:
  - Reducing the code's redundancy (which favors larger overlapping neighborhoods),
  - With **low decoding complexity** and **High recovery power** (which favors smaller overlaps).

- He introduced the concept of **density**, which is a measure of how many bits participate in each parity check:
  - **Higher density** means more overlap (stronger recovery),
  - **Lower density** means less overlap (faster decoding).

:::{.smaller}
## Low Density

::: {.incremental}

- Gallager showed that even **sparse** parity-check matrices can achieve strong error correction, by using efficient **decoding algorithms** like belief propagation.

- Sparsity reduces complexity while still maintaining performance close to capacity.

- Gallager also showed that **randomly generated** sparse matrices usually perform well.

- Randomness avoids problematic patterns in the code structure.

- In practice, carefully designed random LDPC matrices approach the Shannon limit.

:::

:::

## LDPC Theory

- A LDPC code can be presented as a parity-check matrix $H$.

- The matrix is composed of $n$ columns and $k$ rows, where each column represents a bit (pairity bit or regular bit), and each row represents a parity-check equation over some bits.

- Each row has only a few non-zero entries, which mean that the pairity checks have low Density.

- A vector $\bar{v}\in \{0,1\}^{nX1}$ is a codeward if $Hv=\bar{0}$.

### Example

Let $n = 8$, and $k = 3$. Then one possible $H$ matrix:

$$
H =
\begin{bmatrix}
1 & 1 & 0 & 1 & 0 & 0 & 0 & 0\\
0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 & 1 & 1 & 0
\end{bmatrix}
$$

---

## Example - cont. {.smaller}

We use the same parity-check matrix:

$$
H =
\begin{bmatrix}
1 & 1 & 0 & 1 & 0 & 0 & 0 & 0\\
0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 & 1 & 1 & 0
\end{bmatrix}
$$

Let’s check if the vector

$$
c =
\begin{bmatrix}
1 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 1 \\ 0
\end{bmatrix}
$$

is a codeword by computing $Hc$:

---

### Computation {.smaller}

We compute:

<div style="font-size:0.6em;">

$$
Hc =
\begin{bmatrix}
1 & 1 & 0 & 1 & 0 & 0 & 0 & 0\\
0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 1 & 1 & 1 & 0
\end{bmatrix}
\begin{bmatrix}
1 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 1 \\ 0
\end{bmatrix}
$$

</div>

$$
\Downarrow
$$

<div style="font-size:0.8em;">
\begin{align*}
1\cdot x_1 + 0\cdot x_2 + 0\cdot x_4 &= 1 + 0 + 0 = 1 \\
0\cdot x_2 + 1\cdot x_3 + 0\cdot x_8 &= 1 + 0 + 0 = 1 \\
1\cdot x_1 + 0\cdot x_5 + 1\cdot x_6 + 1\cdot x_7 &= 1 + 0 + 1 + 1 = 1
\end{align*}


</div>

<br>
So this is **not** a codeword, because $Hc \neq 0$.

---

## LDPC Encoding

- We introduced LDPC codes using a **parity-check matrix** $H$, where a vector $\bar{v}$ is a **codeword** if it satisfies:
$$
H\bar{v} = 0.
$$

- Our goal now is to generate such codewords from raw data — in other words, to define an **encoding matrix** $G$ such that:
$$
\bar{v} = G\bar{d}
$$
for some data vector $\bar{d}$, and the result $\bar{v}$ is guaranteed to satisfy $H\bar{v} = 0$.

- Since any encoded vector $\bar{v}$ is a **linear combination of the columns of $G$**, This means that the set of all possible codewords is exactly the **column space** of $G$.
  
- Therefore, it must hold $HG=\{0\}$

---

- A common way to construct such a $G$ is to first put $H$ into **systematic form**:
$$
H = \left[ P \,\middle|\, I_m \right]
$$
where $P$ is a matrix of size $m \times k$ and $I_m$ is the identity matrix of size $m = n - k$.

- Then, the generator matrix $G$ is defined as:
$$
G = \left[ I_k \,\middle|\, P^\top \right]
$$
where $I_k$ is the identity matrix of size $k$ and $P^\top$ is the transpose of $P$.


## LDPC - Decoding Introduction

- Checking if a vector is a valid codeword via matrix multiplication with $H$ can be computationally expensive.

<br>

- However, **$H$ is sparse** in LDPC codes.

<br>

- We can exploit this sparsity by representing $H$ as a **graph structure**, enabling more efficient decoding.

<br>

- We present the concept of tanner graphs.

---

## Tanner Graphs: Definition

- A **Tanner graph** is a **bipartite graph** with:
  - **Variable nodes**: one for each bit (column of $H$)
  - **Check nodes**: one for each parity-check (row of $H$)
- An edge connects variable node $v_j$ and check node $c_i$ if and only if $H_{i,j} = 1$.

![](assets/tanner.PNG){width=900px height=400px fig-align="center"}

---

## Tanner Graphs : Example

![](assets/tanner.PNG){width=900px height=400px fig-align="center"}


<div class="fragment">
The corresponding matrix is:
$$
H =
\begin{bmatrix}
1 & 1 & 0 & 0 & 1 & 0 \\
0 & 0 & 1 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 1
\end{bmatrix}
$$

</div>

---

## Gallager’s Bit-Flipping Algorihm

- Bit-Flipping is an iterative decoding algorithm, presented by Gallagher. <br> <br>
- Bit-Flipping is used for decoding LDPC codes over a **Binary Symmetric Channel (BSC)**. <br><br>
- The core idea of the algortihm is to flip bits that participate in **many unsatisfied parity checks**.

---

## Algorithm Overview

1. **Input**: Received vector $\bar{r}$ (possibly corrupted), and parity-check matrix $H$.
2. **Check**: For each parity check, compute whether it's satisfied usign Tanner graph.
3. **Count**: For each bit, count how many unsatisfied checks it participates in.
4. **Flip**: Bits involved in "enough" unsatisfied checks are flipped iteratively.
5. **Repeat**: Iterate until all checks are satisfied or a max number of steps is reached.

---

## Example

Let:
$$
H =
\begin{bmatrix}
1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 1 &  \,0
\end{bmatrix}
\quad \bar{r} = [0\ 1\ 0\ 0\ 1\ 0\ 0]
$$

And the Tanner graph:

![](assets/tanner2.PNG){width=800px height=270px fig-align="center"}


---

## Step 1: Compute Syndromes

We check whether the pairity checks are satisfied:
![](assets/tanner3.PNG){width=800px height=400px fig-align="center"}

Almost all checks are unsatisfied! We count how many unsatisfied checks each bit participates in.

---

## Step 2: Flip Bits

![](assets/tanner3.PNG){width=500px height=200px fig-align="center"}

<div style="font-size:0.8em;">

- Bit 1 appears in 1 unsatisfied check  
- Bit 2 appears in 2 unsatisfied checks.
- Bit 3 appears in 1 unsatisfied check. 
- Bit 4 appears in 1 unsatisfied check.
- Bit 5 appears in 3 unsatisfied checks.
- Bit 6 appears in 1 unsatisfied check.
- Bit 7 appears in 1 unsatisfied checks.

Then we flip bit 5.

</div>


---

## Step 2: Flip Bits

![](assets/tanner4.PNG){width=520px height=220px fig-align="center"}

<div style="font-size:0.8em;">
- Bit 1 appears in 1 unsatisfied check  
- Bit 2 appears in 3 unsatisfied checks.
- Bit 3 appears in 2 unsatisfied check. 
- Bit 4 appears in 1 unsatisfied check.
- Bit 5 appears in 1 unsatisfied checks.
- Bit 6 appears in 0 unsatisfied check.
- Bit 7 appears in 0 unsatisfied checks.

Then we flip bit 2.

</div>

---


## Step 2: Flip Bits

![](assets/tanner5.PNG){width=800px height=400px fig-align="center"}

This is Our Codeword!



---

## Summary

- Bit-Flipping is simple and intuitive.  
- Works well with **sparse LDPC matrices**.  
- Not optimal, but useful as a first decoding step or when low complexity is needed.

</span>



## LDPC Decoding: Belief propagation algorithm

::: {.center}
# 6 Encoding/Decoding Schema {data-name="Schema"}
:::

## 3 challenges in using DNA for data storage
| **Error Type**              | **Solution**                                                               |
|----------------------------|---------------------------------|
| 1. Substitution errors      | • LDPC codes                                                                 |
| 2. Unordered reads          | • Addressing index<br>• BCH codes                                            |
| 3. Insertion/deletion errors | • Convert to substitution errors:<br> • Sync markers<br> • MSA (via indexed clusters) |

<!-- TODO(Matan): Add Index and MSA explanation -->

## Index (+ BCH)

## Multiple Sequence Alignment (MSA)

## Synchronization Marker

- **Problem:** Insertions and deletions ("indels") shift the alignment of bases, making decoding difficult.
- If a read has unexpected length (due to indels), we try to **recover the marker** using Multiple Sequence Alignment (MSA).
- If the marker is located:
  - We **split** the read at the marker.
  - Retain only the expected-length portion.
  - Mark the rest as **erasures**.
- These erasures are then passed into the LDPC decoder.
- In simulations: 10% improvement in the reading cost while having little impact (2-3%) on the writing cost.

::: {.r-vstack style="align-items:center; justify-content:center; gap:6px;"}

<!-- ─────────── 1 ►  COLOURED BAR  ─────────── -->
::: {.r-hstack style="align-items:center; justify-content:center;"}

::: {data-id="payloadL"
     style="background:#ff9f1c;  width:200px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;"}
Payload
:::

::: {data-id="sync"
     style="background:#38b000;  width:100px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;"}
AGT
:::

::: {data-id="payloadR"
     style="background:#ff9f1c;  width:200px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;"}
Payload
:::
:::
:::

## Encoding a single binary file {auto-animate=true auto-animate-easing="ease-in-out"}

- File size: $192_\text{KB} = 192 \times 1000_\text{B} = 1 536 000_\text{bits}$
- Encoding steps from: 
[https://github.com/shubhamchandak94/LDPC_DNA_storage](https://github.com/shubhamchandak94/LDPC_DNA_storage/blob/master/dna_storage.py)
- KB is decimal (1000 bytes = 1 KB), not binary (1024 bytes = 1 KiB)

::: {.r-hstack style="align-items: center; justify-content: center;"}
::: {data-id="strand1"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand2"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand3"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand4"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand5"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand6"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
:::

<!-- ------------- SLIDE 2 ------------- -->
## Large block LDPC encoding {auto-animate=true auto-animate-easing="ease-in-out"}

- LDPC data-block size (`LDPC_dim=256K`): $256 000_\text{bits}$
- Number of data blocks: $\frac{1 536 000}{256 000} = 6$
- Added parity bits (`LDPC_alpha=0.5`): For each data block, we add $128 000_\text{bits}$ of parity bits. 
- Encoded bits per block: $$256 000_\text{bits} + 128 000_\text{bits} = 384 000_\text{bits}$$

::: {.r-hstack style="align-items:center; justify-content:center;"}

<!-- container keeps the total width at exactly 700 px -->
::: {.r-hstack style="width:700px; gap:0px;"}

<!-- 6 orange data pieces -->
::: {data-id="strand1"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand2"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand3"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand4"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand5"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand6"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  

<!-- 6 blue parity pieces -->
::: {data-id="strand7"  style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand8"  style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand9"  style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand10" style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand11" style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand12" style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  

:::
:::

<!-- ───────────────── SLIDE 3 ───────────────── -->
## Segment and  map to DNA {auto-animate=true auto-animate-easing="ease-in-out"}

- Binary mapping: $00 \to A$, $01 \to C$, $10 \to G$, $11 \to T$
- Bits per oligo (`payload size=84bp`): $84 \times 2_\text{bits} = 168_\text{bits}$
- Number of oligos per block: $$\frac{\text{payload bits}}{\text{bits per oligo}}=\frac{384 000_\text{bits}}{168_\text{bits}} = 2285.71 \approx 2286_\text{oligos}$$
- Total number of oligos: $\text{blocks} \times \text{oligos per block} = 6 \times 2286 = 13716$ (✔)

::: {.r-hstack style="gap:60px;justify-content:center;"}

<!-- ─────────── column 1 ─────────── -->
::: {.r-vstack style="gap:10px;"}
::: {data-id="strand1" style="background:#ff9f1c;width:230px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand2" style="background:#ff9f1c;width:230px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand3" style="background:#ff9f1c;width:230px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand4" style="background:#ff9f1c;width:230px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand5" style="background:#ff9f1c;width:230px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand6" style="background:#ff9f1c;width:230px;height:24px;border-radius:4px;"}
:::
:::

<!-- ─────────── column 2 ─────────── -->
::: {.r-vstack style="gap:10px;"}
::: {data-id="strand7"  style="background:#1982c4;width:165px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand8"  style="background:#1982c4;width:165px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand9"  style="background:#1982c4;width:165px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand10" style="background:#1982c4;width:165px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand11" style="background:#1982c4;width:165px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand12" style="background:#1982c4;width:165px;height:24px;border-radius:4px;"}
:::
:::
:::


## Encoded oligo structure (/W Sync markers) {auto-animate=true auto-animate-easing="ease-in-out"}
| Segment                             | bp         |
| ----------------------------------- | ---------- |
| BCH-protected index                 | $14_\text{bit} / 2 = 7_\text{bp}$      |
| BCH redundancy      | $12_\text{bit} / 2 = 6_\text{bp}$       |
| Sync marker (`AGT`)            | $3_\text{bp}$       |
| **Payload** | $84_\text{bp}$      |
| Primers                   | $2 \times 25_\text{bp}$       |
| **Total**                            | **$150_\text{bp}$**  |

::: {.r-vstack style="align-items:center; justify-content:center; gap:6px;"}

<!-- ─────────── 1 ►  COLOURED BAR  ─────────── -->
::: {.r-hstack style="align-items:center; justify-content:center;"}

::: {data-id="primerL"
     style="background:#6c757d;  width:100px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;
            border-top-left-radius:6px; border-bottom-left-radius:6px;"}
FW
:::

::: {data-id="indexECC"
     style="background:#d90429;  width:200px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;"}
Index + ECC
:::

::: {data-id="payloadL"
     style="background:linear-gradient(135deg,
                   #ff9f1c 0%,   /* orange in the upper-left corner   */
                   #1982c4 100%);/* blue in the lower-right corner    */
       width:200px; height:60px;
       color:#fff; font-weight:bold;
       display:flex; align-items:center; justify-content:center;"
}
Payload
:::

::: {data-id="sync"
     style="background:#38b000;  width:100px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;"}
AGT
:::

::: {data-id="payloadR"
     style="background:linear-gradient(135deg,
                   #ff9f1c 0%,   /* orange in the upper-left corner   */
                   #1982c4 100%);/* blue in the lower-right corner    */
       width:200px; height:60px;
       color:#fff; font-weight:bold;
       display:flex; align-items:center; justify-content:center;"
}
Payload
:::

::: {data-id="primerR"
     style="background:#6c757d;  width:100px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;
            border-top-right-radius:6px; border-bottom-right-radius:6px;"}
REV
:::
:::

<!-- ─────────── 2 ►  bp LABEL ROW  ─────────── -->
::: {.r-hstack style="gap:0px; font-size:0.8em; font-weight:bold; color:#000;"}

::: {data-id="primerL_lbl"   style="width:100px; text-align:center;"}
25 bp
:::

::: {data-id="indexECC_lbl"  style="width:200px; text-align:center;"}
6+7 bp
:::

::: {data-id="payloadL_lbl"  style="width:200px; text-align:center;"}
42 bp
:::

::: {data-id="sync_lbl"      style="width:100px; text-align:center;"}
3 bp
:::

::: {data-id="payloadR_lbl"  style="width:200px; text-align:center;"}
42 bp
:::

::: {data-id="primerR_lbl"   style="width:100px; text-align:center;"}
25 bp
:::
:::
:::

## Encoding schema overview

::: {.r-stack style="height:80vh; display:flex; flex-direction:column; justify-content:center; align-items:center;"}
![](assets/encoding.png)
:::

## Decoding schema overview
<!-- TODO(Mattan) -->


::: {.center}
# 7 Experimental results {data-name="Results"}
:::

## Write/read costs (Fig. 8 / Table 1) {.smaller}

<!-- Table block -->
<div style="position:relative;">

| Exp. No. | LDPC Redundancy | File Size | No. of Oligonucleotides | Normalized Coverage Variance | Writing Cost (bases/bit) | Reading Cost (bases/bit) |
|----------|------------------|-----------|--------------------------|-------------------------------|---------------------------|---------------------------|
| 1        | 50%              | 160 KB    | 11,710                   | 1.97                          | 0.91                      | 2.73                      |
| 2        | 10%              | 224 KB    | 12,026                   | 1.57                          | 0.67                      | 3.82                      |
| 3        | 50%              | 192 KB    | 13,716                   | 3.36                          | 0.89                      | 3.45                      |
| 4        | 30%              | 192 KB    | 11,892                   | 3.53                          | 0.78                      | 4.46                      |
| 5        | 10%              | 192 KB    | 10,062                   | 3.19                          | 0.66                      | 8.11                      |

<!-- Floating figure -->
<div class="fragment"
     style="position:absolute; top:50px; right:-60px; width:790px; z-index:10;">
  <img src="assets/figure8.png" style="width:150%;">
</div>

</div>


## Error profile (Fig. 9)
::: {.r-stack style="height:80vh; display:flex; flex-direction:column; justify-content:center; align-items:center;"}
![](assets/figure9.png)
:::

## Coverage profile (Fig. 10)
::: {.r-stack style="height:80vh; display:flex; flex-direction:column; justify-content:center; align-items:center;"}
![](assets/figure10.png)
:::


## Indel correction heuristics (Table. 4)
<!-- | LDPC Redundancy | Writing Cost | Reading Cost (1 Indel) | Reading Cost (0 Indels) |
|------------------|---------------|--------------------------|--------------------------|
| 50%              | 0.89          | 3.45                     | 3.58                     |
| 30%              | 0.78          | 4.46                     | 4.73                     |
| 10%              | 0.66          | 8.11                     | 8.57                     | -->

<div style="center; position:relative;">
![](assets/table4.png){fig-align="center" width="80%"}
</div>

- Attempting to correct single indel with a BCH code
- This step is able to correct 5-10% additional indexes.

## Probability of dec. failure (Fig. 11)
::: {.r-stack style="height:80vh; display:flex; flex-direction:column; justify-content:center; align-items:center;"}
![](assets/figure11.png)
:::

<!-- TODO(Matan): Add a figure with the probability of decoding failure -->
## Stress testing

* Simulated **6% total error** (2% sub / 2% del / 2% ins)
* Added **15% random reads** (unaligned noise)
* **224 KB** file, **50% LDPC**, BCH (3-error correction)
* Increased LDPC decoder threshold to **10%**
* **Write cost**: 1.07 bases/bit
* **Decoding succeeded** at **10.5 bases/bit read cost**


::: {.center}
# 8 Concluding Remarks {data-name="Conclusions"}
:::

## Conclusions

* Achieves **better read/write cost tradeoff** than prior work
* Combines **LDPC codes** with **heuristics** for indel correction
* Insights may help improve **bioinformatics tools** and **error models**

## Future Work

* Use **channel-optimized LDPC** or **marker codes**
* Extend to **nanopore sequencing** (high indel rates)
* Improve **index error correction efficiency**

## Limitations
* **Random access** No range queries or selective decoding
* **Counting on heuristics** for indel correction, which may not generalize well
* **Error model** Assumes independent errors, which may not hold in practice
* **Relying on simulations** for performance evaluation, which may not fully capture real-world complexities

<!-- TODO(Atar & Matan): Think of improvments -->



## Crafting the Presentation: Tools
- [`Quarto`](https://quarto.org/): markdown-based authoring system that supports multiple output formats.
- [`revealjs`](https://revealjs.com/): a framework for creating interactive presentations using HTML and JavaScript.
  - [`simplemenu`](https://github.com/Martinomagnifico/quarto-simplemenu): a plugin to create a menu bar that allows us to navigate through the presentation.

## Ideas

- Gallager youtube talk at the end of the slides
- Example of LDPC using [pyldpc](https://hichamjanati.github.io/pyldpc/auto_examples/plot_coding_decoding_simulation.html#sphx-glr-auto-examples-plot-coding-decoding-simulation-py)