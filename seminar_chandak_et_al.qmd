---
title: "Seminar on DNA Data Storage"
subtitle: "Improved read/write cost tradeoff in DNA-based data storage using LDPC codes<br><br>Shubham Chandak, Kedar Tatwawadi, Billy Lau, Jay Mardia, Matthew Kubit, Joachim Neu, Peter Griffin, Mary Wootters, Tsachy Weissman, Hanlee J (2019)<br><br>Presented by: Atar Ron and Mattan Hoory"
format:
  revealjs: 
    title-slide-attributes:
      data-state: "hide-menubar"
    slide-number: true
    preview-links: auto
    css: style.css
    logo: assets/CS_LOGO.jpg
    footer: 'DNA Data Storage - 02360801 - Spring 2025'
    toc: true
    toc-depth: 1
    simplemenu:
        flat: true
        barhtml:
            header: "<div class='menubar'><ul class='menu'></ul><div>"
        scale: 0.42

revealjs-plugins:
  - simplemenu
---
  
::: {.center}
# 1 Introduction & Context {data-name="Intro"}
:::

## Background

<span style="font-size:0.7em"> In DNA-based data storage, there are two critical challenges: </span>

::: {.incremental}

- <span style="font-size:0.7em">**Write cost** — how much synthetic DNA we need to store one bit of information. </span>
- <span style="font-size:0.7em">**Read cost** — how many reads are needed to reliably recover that bit.</span>


<span style="font-size:0.7em" class= "fragment"> These two costs are closely related, and thus, one of the most important problems in DNA data storage is finding an effective tradeoff between them. </span>

<div style="text-align: center;">
  <img src="assets/write_read_proc.PNG" width="400px" height="200px" class="fragment">
</div>

:::

---

## Discussion Questions

::: {.incremental}

- Why , in your opinion, are the cost of write and cost of read closely related? how does the write cost affect the read cost?
&nbsp; &nbsp;  
- What are the main factors that affect the cost of write?  
&nbsp; &nbsp;  
- What are the main factors that affect the cost of read?

:::


---

## Main Goals

::: {.incremental}

Since the cost of write and cost of read are strongly correlated, this paper aims to:


- Establish a theoretical lower bound on the tradeoff between write cost and read cost.
- Design and evaluate a practical coding scheme (based on LDPC codes) that achieves a better tradeoff than previous methods.
- Validate the performance of the scheme through both real experiments (DNA synthesis and sequencing) and simulations.

:::

## Cost of Read and Write 

* **Cost Of Write**
  Average number of encoded bits synthesized per information bit:

  $$
  c_w = \frac{\text{#Synthesized bits}}{\text{#Data}}
  $$

* **Cost of Read**
  Average number of bits read per information bit:

  $$
  c_r = \frac{\text{#Read bits}}{\text{#Data bits}}
  $$

## Coverage

* <span style="font-size:0.7em">**Coverage**- Average number of bits read per synthesized bit:</span>


<div style="font-size: 0.8em;">


  $$
  \text{Coverage} = \frac{\text{#Read Bits}}{\text{#Synthesized bits}}
  = \frac{\frac{\text{#Read bits}}{\text{#Data bits}}}{\frac{\text{#Synthesized bits}}{\text{#Data bits}}}
  = \frac{c_r}{c_w}
  $$

</div>


::: {.incremental}
- <span style="font-size:0.7em">Coverage has been widely used in prior work to estimate the efficiency of read/write tradeoffs in DNA-based storage systems. 
<br> </span>

- <span style="font-size:0.7em">It measures how many sequencing reads are made per synthesized bit — but is that the best way to evaluate system performance?
</span>

:::
---

## Is Coverage a Good Metric?

<span style="font-size:0.7em">We consider the following example that demonstrates why coverage can be misleading.</span><br>

### Example
<span style="font-size:0.7em">Suppose we compare two storage systems with the following properties:</span>

| System        | $c_w$ | $c_r$ | $\text{coverage} = c_r/c_w$ |
|----------------|------------|----------------|------------------------|
| A  | 4                   | 12                 | 3                   |
| B  | 2                   | 10                 | 5                   |

* <span style="font-size:0.7em">**Note:** In this example, we assume that both the read cost and the coverage values were measured after decoding all the sampled strands.</span>



## Example - cont. 

| System        | $c_w$ | $c_r$ | $\text{coverage} = c_r/c_w$ |
|----------------|------------|----------------|------------------------|
| A  | 4                   | 12                 | 3                   |
| B  | 2                   | 10                 | 5                   |

* <span style="font-size:0.8em">At first glance, System A has better (lower) coverage. Therefore, when evaluating the systems based on coverage only, we will prefer System A. </span>  


* <span style="font-size:0.8em"> But, it is easy to see that System B reads fewer total bits per information bit (10 vs. 12), and also synthisizes significantly less DNA. So despite having higher coverage, System B is clearly more efficient overall. </span>


---

## Example - cont. 
<br>
**Conclusion:** <span style="font-size:0.9em">This example illustrates that relying solely on coverage can be misleading, particularly when comparing systems with varying write costs. A more meaningful comparison is to evaluate the actual read and write costs per information bit.</span>  
<br>

**Important Note:** <span style="font-size:0.9em">We can also define coverage at the strand level: the average number of times a strand is observed in the sampled reads. We'll refer back to this concept later on.</span>

::: {.center}
## The Model
:::

## Model Notations

- $n$ – Number of strands.  


- $L$ – Strand length. 


- $c_{w}$ – Cost of write. 


- $c_{r}$ – Cost of read.

- $\epsilon$ - Substitution error rate.


## Model Definition {.nostretch}

::: {.fragment data-fragment-index="1"}
- <span style="font-size:0.7em; color:#00B0F0; line-height: 1.0; display: inline-block; margin-bottom: 4px;">
    The storage system encodes $n$ information strands, each of length $L$, resulting in a total of $nL$ data bits.
  </span>
:::

::: {.fragment data-fragment-index="3"}
- <span style="font-size:0.7em; color:#059B2C; line-height: 1.0; display: inline-block; margin-bottom: 4px;">
    Each data bit leads to the synthesis of $c_w$ bits. Thus, encoding $nL$ data bits requires synthesizing $nc_wL$ bits, which are grouped into $nc_w$ strands.
  </span>
:::

::: {.fragment data-fragment-index="5"}
- <span style="font-size:0.7em; color:#059B2C; line-height: 1.0; display: inline-block; margin-bottom: 4px;">
    Reading each data bit involves sampling $c_r$ synthesized bits. Therefore, $nL$ data bits require sampling of $nc_rL$ synthesized bits, which can be organized into $nc_r$ strands.
  </span>
:::


<!-- Images side-by-side and controlled with fragments -->
<div style="margin-bottom: 200px;">
  <img src="assets/model_process1.PNG" width="280" height="200" class="fragment" data-fragment-index="2">
  <img src="assets/model_process2.PNG" width="350" height="200" class="fragment" data-fragment-index="4">
  <img src="assets/model_process3.PNG" width="370" height="200" class="fragment" data-fragment-index="6">
</div>


---


## Model Definition - cont.

<div style="font-size:0.8em; line-height: 1.2;">
- It is assumed, for simplicity, that the decoder has access to the index of each strand, and that deletion and insertion errors are ignored. We later explain how the authors overcame this in practice. 

- The reads are subject to:

  - <strong>Substitution errors</strong> — each sampled bit is flipped with probability $\epsilon$ (modeled as a BSC, discussed later).
  - <strong>Sampling variability</strong> — the number of times each strand is sampled is random and follows a Poisson distribution.

![](assets/model_process4.PNG){width=800px height=150px fig-align="center"}

</div>


---

## Data Distribution

<div style="font-size:0.8em; line-height: 1.2;">

- For the sake of the theoretical analysis, it is assumed that the sampled DNA strands follow a Poisson distribution.
- Does not account for synthesis or sequencing biases, but still provides significant insights into strand sampling behavior.

#### Poisson Recap:
- The Poisson distribution models the number of times an event occurs in a fixed interval, given a known average rate $\lambda$.
- The probability mass function is given by:

</div>

<div style="font-size:0.8em">
$$
P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}
$$

</div>


## Why Poisson?

::: {.incremental}

<div style="font-size:0.7em; line-height: 1.1;">

- The authors assume that each of the $n \cdot c_r$ sequenced strands is selected independently and uniformly at random from the $n \cdot c_w$ synthesized strands — with replacement.</span>

- This results in a **multinomial distribution**: each of the $n \cdot c_w$ strands has a fixed probability $\frac{1}{n \cdot c_w}$ of being chosen in any given draw.</span>

- When the number of synthesized strands is large, the probability of selecting any particular strand becomes very small, but the number of trials remains large.</span>

- In this case, the multinomial distribution can be approximated with **independent Poisson distributions** where:</span>
  Each strand is independently selected according to a $\text{Poisson}(\lambda)$ distribution where:

</div>


::: {.fragment}
<div style="font-size: 0.7em">
$$
\lambda = \text{#Samples} \cdot \text{Probability} = \frac{nc_r}{nc_w} = \frac{c_r}{c_w}
$$
</div>
:::


:::


::: {.center}
# 2 Communication and Information 101 {data-name="Theory"}
:::

---


## Communication and Information - Motivation

<div style="font-size:0.9em; line-height: 1.7;">

- We’ve seen that the **write cost** $c_w$ and **read cost** $c_r$ are tightly connected. With an optimal code, increasing $c_w$ typically reduces $c_r$.

- However, a higher write cost means more redundancy, which **lowers the information rate** per bit.

- In this section, we introduce basic **channel theory** to better understand and analyze the **tradeoff between $c_w$ and $c_r$**.

</div>

::: {.notes}
- We have shown that the cost of read and cost of write are closely related, and that the coverage is not a good metric to evaluate the tradeoff between them.
- To understand how, we need to understand the theoretical foundations of communication and information theory.
:::
---

## What is a Channel?


<div style="font-size:0.9em; line-height: 1.2;">

A **channel** is a mathematical model used in information theory to describe how information is transmitted from a sender to a receiver.

- Input $X$: a message of n bits.
- Output $Y$: a possibly altered version of the message

The channel introduce **noise**, leading to errors or loss.

</div>

![](assets/channel.PNG){width=600px height=250px fig-align="center"}

---

## Channel Examples 
- <span style="font-size:0.7em;">Binary Erasure Channel (BEC):</span>  
  <div style="margin-left: 400px; margin-top: -10px; margin-bottom: 5px; text-align: center;">
    <img src="assets/BEC_example.PNG" width="300px" height="200px" style="display: inline-block;">
  </div>

- <span style="font-size:0.7em;">Binary Symmetric Channel (BSC):</span>  
  <div style="margin-left: 400px; margin-top: -10px; margin-bottom: 5px; text-align: center;">
    <img src="assets/BSC.PNG" width="300px" height="200px" style="display: inline-block;">
  </div>

<!--
## Entropy

- <span style="font-size:0.8em">**Entropy** quantifies the uncertainty or randomness in a random variable. </span>
- <span style="font-size:0.8em">Formally, if a discrete random variable $X$ takes values $x_1, \dots, x_n$ with probabilities $p_1, \dots, p_n$ , then:</span>

$$
H(X) = -\sum_{i=1}^n p_i \log_2 p_i
$$

- <span style="font-size:0.8em">The entropy is always between 0 and 1: $$0 \leq H(X) \leq 1$$</span>
- <span style="font-size:0.8em">Higher entropy means more uncertainty.</span>
- <span style="font-size:0.8em">Lower entropy means the variable is more predictable.</span>

---

## Entropy – Cont.

- $H(X) = 0$ when $X$ is completely predictable (e.g., a constant value).
- $H(X) = 1$ when $X$ is completley uncertain. This occurs when $X \sim \text{Bern}(\tfrac{1}{2})$.
- Entropy satisfies a version of the law of total probability:

$$
H(X) = \sum_y P(Y = y) \cdot H(X \mid Y = y)
$$

![](assets/entropy.PNG){width=600px height=300px fig-align="center"}

---

## Capacity of a Binary Erasure Channel (BEC)

<span style="font-size:0.7em"> Assume a bit $X\sim\mathrm{Bern}\bigl(\tfrac12\bigr)$ and a Binary Erasure Channel with erasure probability $\varepsilon$. Let $Y$ be the output of the channel. Then:</span>

- <span style="font-size:0.7em"> The entropy of $X$ holds: $H(X)=1$</span>
- <span style="font-size:0.7em"> The conditional entropy of $X$ given $Y$ is:</span>

::: {.small-math}
$$  
\begin{aligned}  
H(X \mid Y)
&= P(Y = ?) \cdot H(X \mid Y = ?) + P(Y \neq ?) \cdot H(X \mid Y \neq ?) \\
&= \varepsilon \cdot 1 + (1 - \varepsilon) \cdot 0 \\
&= \varepsilon
\end{aligned}
$$

:::

- <span style="font-size:0.7em">This holds because when the output is "?"", the input is completely uncertain, i.e., $P(X=1) = P(X=0) = 0.5$. In contrast, when the output is 0 or 1, the input is known with certainty, as defined by the Binary Erasure Channel (BEC).</span>


---

## Capacity of a Binary Erasure Channel (BEC)
**<span style="font-size:1.1em">Mutual information </span>** 

- Mutual information is a measure of how much information the output $Y$ gives on the input $X$.

- It is defined as:
   $$  
   \begin{aligned}  
   I(X;Y)
   &=H(X)\;-\;H(X\mid Y)\\  
   \end{aligned}
   $$

  - In our case: 

    $$  
   \begin{aligned}  
   I(X;Y)
   &=H(X)\;-\;H(X\mid Y)=\\
   &=1\;-\;\varepsilon  
   \end{aligned}
   $$

---

-->

## Capacity of a Binary Erasure Channel (BEC)
  

<div style="font-size:0.9em; line-height: 1.2;">

- Capacity is the highest rate at which information can be reliably transmitted over a communication channel.<br><br>

- It quantifies the maximum rate that can be recovered reliably from the channel's output, per transmitted bit.<br><br>

- For a Binary Erasure Channel (BEC) with erasure probability $\varepsilon$, the capacity is:  
$$
C = 1 - \varepsilon
$$

</div>

---

## Code Rate

<div style="font-size:0.8em; line-height: 1.1;">

- In coding theory, the **code rate** $R$ quantifies the rate of **information** transmitted per **encoded bit**.

- Recall that the write cost $c_w$ is defined as:  
  $$
  c_w = \frac{\text{#Synthesized bits}}{\text{#Data bits}}
  $$
  which implies:
  $$
  \frac{1}{c_w} = \frac{\text{#Data bits}}{\text{#Synthesized bits}} = R
  $$

- In other words, the **inverse write cost** captures how much information is packed into each encoded bit, which is the code rate.

</div>

---

## Shannon's Theorem

<div style="font-size:0.9em; line-height: 1.1;">

- <u>Shannon’s Theorem</u>: Reliable communication over a noisy channel is possible **if and only if** the code rate $R$ satisfies $R \leq C$, where $C$ is the channel’s capacity.<br>
<br>
- Intuitively, if $R > C$, then the rate of inforamtion in each bit is higher than the rate that can be reliably recovered. Therefore, some of it must be lost or corrupted.<br>
<br>
- However, if $R \leq C$, Shannon proved that with a suitable coding scheme and long enough messages, the error probability can be made arbitrarily small.

</div>

---

## Numerical Example

Assume we have a BEC with $\varepsilon=0.6$ and an encoder with $c_w=2$.

- $R=\frac{1}{c_w}=\frac{1}{2}$.
- $C=1-\varepsilon = 1-0.6=0.4$
- It holds that $R=0.5>0.4$
- Therefore, in this model, the data might not be decoded correctly, or might miss some information.

---

## Recap

<div style="font-size:0.8em; line-height: 1.2;">
- The storage model consists of $nc_w$ synthesized strands and $nc_r$ sampled strands. Each strand is sampled a number of times drawn from a $\text{Poisson}(\lambda)$ distribution. The index of each strand is assumed to remain intact.


- The data is transmited through a Binary Symmetric Channel (BSC) with substitution rate $\varepsilon$.

- In a Binary Erasure Channel (BEC), the capacity of the channel must be greater or equal to the rate of the code. 

</div>

![](assets/full_proc.PNG){width=1000px height=180px fig-align="center"}



    
::: {.center}
# 3 Theoretical bounds {data-name="Bounds"}
:::

## The Case $\varepsilon = 0$

<div style="font-size:0.7em; line-height: 1.0;">

- When $\varepsilon = 0$, there are **no substitution errors**.<br><br>

- Each DNA strand is sampled independently, and the number of times a strand appears follows a **Poisson distribution** with mean $\lambda = \frac{c_r}{c_w}$.<br><br>

- The probability that a given strand is sampled 0 times is $\mathbb{P}[X=0] = e^{-\lambda}$. <br><br>

- If a strand is not observed in the read process, we effectively have an **erasure**.<br><br>

- Thus, the model can be modeled as a **Binary Erasure Channel (BEC)** with erasure probability: $\varepsilon =e^{-c_r / c_w}$.

</div>

---

## The Case $\varepsilon = 0$ - cont.

<div style="font-size:0.7em; line-height: 1.2;">

- Since the capacity of the BEC must be greater or equal to the rate of the code, we get that: 

$$
R=\frac{1}{c_w}\leq 1-e^{-\frac{c_r}{c_w}}= C
$$

Simplifying the equation, we get the following lower bound for the cost of read:

$$
c_r\geq c_w ln(\frac{c_w}{c_w-1})
$$

- We can see that as $c_w$ increases, $c_r$ decresed, and vice-versa. This fits our intuition. 

</div>


## The Case $\varepsilon \neq 0$ 

<div style="font-size:0.7em; line-height: 1.4;">

- Now, each sampled bit is transmitted through a BSC with error probability $\varepsilon>0$.<br><br>

- Recall that it is assumed that the index of each strand remains intact, and that each strand is sampled $k$ times, where $k$ is $Poisson(\frac{c_r}{c_w})$ distributed.<br><br>

- Assume that for each bit in each strand, we are given a tuple $(k_0,k_1)$ that indicates how how many times the bit was sampled as 0 and 1, consecutively. Note that $k_0+k_1=k$.<br><br>

- The probability that the samples of the bit are ($k_0,k_1$) given that the bit has value 0 is:

$$
P((k_0,k_1)|0)=\frac{e^{-\lambda}\lambda^{k_0+k_1}}{(k_0+k_1)!} {k_0+k_1\choose k_1} (1-\varepsilon)^{k_0}\varepsilon^{k_1}
$$


</div>

---

## The Case $\varepsilon \neq 0$ - cont.

<div style="font-size:0.8em; line-height: 1.2;">

$$
P((k_0,k_1)|0)=\frac{e^{-\lambda}\lambda^{k_0+k_1}}{(k_0+k_1)!} {k_0+k_1\choose k_1} (1-\varepsilon)^{k_0}\varepsilon^{k_1}
$$

* The term $\frac{e^{-\lambda}\lambda^{k_0+k_1}}{(k_0+k_1)!}$ is the probability that a poisson random variable gets the value $k_0+k_1$. 

* the term ${k_0+k_1\choose k_1} (1-\varepsilon)^{k_0}\varepsilon^{k_1}$ is the binomial probability that there are $k_1$ errors out of $k_0+k_1$ samples. 

* The probability of observing the sample counts $(k_0, k_1)$ given that the bit is 1 is symmetric with respect to swapping $k_0$ and $k_1$.

The following example illustrates how the calculations are performed.

</div>

---

## The Case $\varepsilon \neq 0$ - Example

- <span style="font-size:0.8em"> In this example, we assume that $\epsilon=0.1$ and $\lambda=3$. </span>


::: {.center}

<div style="font-size: 0.8em;">

<table style="font-family: monospace; font-size: 1.1em; border-collapse: separate; border-spacing: 0 6px;">
  <tr style="border-bottom: 2px solid #999;">
    <th style="padding: 0px 12px;">Source</th>
    <th style="padding: 0px 12px;">Bit 1</th>
    <th style="padding: 0px 12px;">Bit 2</th>
    <th style="padding: 0px 12px;">Bit 3</th>
    <th style="padding: 0px 12px;">Bit 4</th>
  </tr>
  <tr>
    <td style="padding: 0px 12px;">Original</td>
    <td style="color: black; text-align: center;">1</td>
    <td style="color: black; text-align: center;">0</td>
    <td style="color: black; text-align: center;">1</td>
    <td style="color: black; text-align: center;">0</td>
  </tr>
  <tr>
    <td style="padding: 0px 12px;">Sample 1</td>
    <td style="color: blue; text-align: center;">1</td>
    <td style="color: red; text-align: center;">0</td>
    <td style="color: blue; text-align: center;">1</td>
    <td style="color: red; text-align: center;">0</td>
  </tr>
  <tr>
    <td style="padding: 0px 12px;">Sample 2</td>
    <td style="color: blue; text-align: center;">1</td>
    <td style="color: blue; text-align: center;">1</td>
    <td style="color: blue; text-align: center;">1</td>
    <td style="color: red; text-align: center;">0</td>
  </tr>
  <tr>
    <td style="padding: 0px 12px;">Sample 3</td>
    <td style="color: red; text-align: center;">0</td>
    <td style="color: red; text-align: center;">0</td>
    <td style="color: blue; text-align: center;">1</td>
    <td style="color: red; text-align: center;">0</td>
  </tr>
  <tr>
    <td style="padding: 0px 12px;">$(k_0,k_1)$</td>
    <td>(1,2)</td>
    <td>(2,1)</td>
    <td>(0,3)</td>
    <td>(3,0)</td>
  </tr>
  <tr>
    <td style="padding: 0px 12px;">Probability</td>
    <td>0.054</td>
    <td>0.054</td>
    <td>0.163</td>
    <td>0.163</td>
  </tr>
</table>

</div>
:::



---

## The Case $\varepsilon \neq 0$ - cont.

<div style="font-size:0.8em; line-height: 1.2;">

- The authors analyze the system from a **bit-level perspective**, rather than over entire strands. <br><br>

- Specifically, they simulate the decoding process and compute the probability of successful recovery **per bit**, conditioned on the known true value (either 0 or 1). This allows detailed numerical estimation of the expected read effort required for reliable decoding.<br><br>

- Since the analysis is performed per-bit and not per-strand, and relies on simulations rather than a closed-form expression, the resulting bound on $c_r$ is **empirical** and  **non-tight**.

</div>



## $c_r$ vs $c_w$: Theoretical bounds 
<br>

![](assets/Graph_1.PNG){width=820px height=450px fig-align="center"}


::: {.center}
# 4 Comparison of coding strategies {data-name="Encoding Strategies"}
:::

## Coding Strategies - Introduction

When designing an encoding and decoding scheme, we must account for **two primary sources of error**:

- **Outer Code Errors** — Since the data is divided into multiple strands, some strands may never be sampled or sequenced. The outer code must handle such **erasure errors** and enable recovery of the missing data.

- **Inner Code Errors** — During synthesis and sequencing, strands may undergo **substitution**, **insertion**, or **deletion** errors. The inner code must correct these errors to recover the original encoded information

## Two types of coding strategies

<!-- TODO (Mattan): Add legends -->

</br></br></br>


::: {.r-hstack style="gap:80px; justify-content:center;"}

<!-- ========== LEFT ◂  Inner + Outer Code Separation ========== -->
::: {.r-vstack style="gap:10px;"}

::: {style="font-weight:bold; text-align:center;"}
Inner / Outer Code Separation
:::

<!-- inner-code strands (orange body + blue tail) -->
::: {style="display:flex; gap:0px;"}
::: {data-id="sp1"  style="background:#ff9f1c; width:120px; height:24px; border-radius:4px 0 0 4px;"}
:::
::: {style="background:#d90429; width:20px; height:24px;"}
:::
::: {data-id="sp1p" style="background:#1982c4; width:90px; height:24px; border-radius:0 4px 4px 0;"}
:::
:::

::: {style="display:flex; gap:0px;"}
::: {data-id="sp2"  style="background:#ff9f1c; width:160px; height:24px;"}
:::
::: {data-id="sp2p" style="background:#1982c4; width:70px; height:24px;"}
:::
:::

::: {style="display:flex; gap:0px;"}
::: {data-id="sp3"  style="background:#ff9f1c; width:160px; height:24px;"}
:::
::: {data-id="sp3p" style="background:#1982c4; width:70px; height:24px;"}
:::
:::

<!-- full-strand erasure (grey) -->
::: {style="display:flex; gap:6px; align-items:center;"}
::: {data-id="spErase" style="background:#6c757d; width:230px; height:24px; border-radius:4px;"}
:::
::: {style="font-size:0.8em; font-weight:bold; color:#6c757d;"}
Erasure
:::
:::

<!-- outer-code strands (green body + blue tail) -->
::: {style="display:flex; gap:0px;"}
::: {data-id="sp4"  style="background:#38b000; width:120px; height:24px; border-radius:4px 0 0 4px;"}
:::
::: {style="background:#d90429; width:20px; height:24px;"}
:::
::: {data-id="sp4p" style="background:#1982c4; width:90px; height:24px; border-radius:0 4px 4px 0;"}
:::
:::

::: {style="display:flex; gap:0px;"}
::: {data-id="sp5"  style="background:#38b000; width:160px; height:24px;"}
:::
::: {data-id="sp5p" style="background:#1982c4; width:70px; height:24px;"}
:::
:::

::: {style="display:flex; gap:0px;"}
::: {data-id="sp6"  style="background:#38b000; width:160px; height:24px;"}
:::
::: {data-id="sp6p" style="background:#1982c4; width:70px; height:24px;"}
:::
:::

:::

<!-- ========== RIGHT ▸  Single Large-Block LDPC ========== -->
::: {.r-vstack style="gap:10px;"}

::: {style="font-weight:bold; text-align:center;"}
Single Large-Block LDPC
:::

<!-- data strands (orange) -->
::: {data-id="ld1" style="background:#ff9f1c; width:230px; height:24px; border-radius:4px;"}
:::
::: {data-id="ld2" style="background:#ff9f1c; width:230px; height:24px;"}
:::

::: {style="display:flex; gap:0px;"}
::: {data-id="ld3" style="background:#ff9f1c; width:180px; height:24px; border-radius:4px 0 0 4px;"}
:::
::: {style="background:#d90429; width:50px; height:24px; border-radius:0 4px 4px 0;"}
:::
:::

::: {data-id="ld4" style="background:#ff9f1c; width:230px; height:24px;"}
:::

<!-- parity strands (blue) -->
::: {data-id="ld5" style="background:#1982c4; width:230px; height:24px; border-radius:4px;"}
:::

::: {style="display:flex; gap:0px;"}
::: {data-id="ld6" style="background:#1982c4; width:180px; height:24px; border-radius:4px 0 0 4px;"}
:::
::: {style="background:#d90429; width:50px; height:24px; border-radius:0 4px 4px 0;"}
:::
:::

::: {data-id="ld7" style="background:#1982c4; width:230px; height:24px;"}
:::
::: {data-id="ld8" style="background:#1982c4; width:230px; height:24px;"}
:::

:::

:::


<!--
## Coding Strategies

- In the DNA-based storage world, there is a variety of coding schemes and techniques to recover inner and outer code errors.

- The authors of the paper consider 2 coding strategies:
  * **Inner/Outer Code Separation** - a coding startegy that is composed of an inner code, which handles error inside strands, and outer code, which handles errors in the order of the strands. 
  * **Single Large Block Code** - a coding strategy where large block of inforamtion us encoded, and then it is segmented into strands. 

- Most of works on DNA-based storage prior to this work used the seperation technique, some incorporating elements from the single large block strategy.


Waiting for Mattan's illustrations of the coding strategies.




## Inner/Outer Code Strategy




::: {.incremental}

- <span style="font-size:0.8em">This strategy uses two layers of coding:

  - <span style="font-size:0.8em">**Outer code**: Applied across information segments to correct **erasures**.
  - <span style="font-size:0.8em">**Inner code**: Applied within each strand to correct **substitution errors**.

- <span style="font-size:0.8em">The decoder groups reads by index and applies **majority voting** per position. Then:
  - <span style="font-size:0.8em">The **inner decoder** corrects strand-level errors.
  - <span style="font-size:0.8em">The **outer decoder** recovers missing segments.

- <span style="font-size:0.8em">This approach is less effective for short blocks, but near-optimal erasure codes exist for large $n$.

:::


![](assets/inner_outer.PNG){width=900px height=180px fig-align="center"}



## Single large block code

::: {.incremental}

- <span style="font-size:0.8em">Instead of using two separate codes, this approach encodes the **entire data block** using one large **LDPC (Low-Density Parity-Check) code**, and then seperates the encoded block into segments. </span>

- <span style="font-size:0.8em">Each segment contains a small portion of a single, long LDPC codeword. </span>

- <span style="font-size:0.8em">During decoding:</span>
  - <span style="font-size:0.8em">All sequenced strands are aligned using their **indices**.</span>
  - <span style="font-size:0.8em">The collected data is treated as a **noisy, partial version** of the LDPC codeword.</span>
  - <span style="font-size:0.8em">A **belief propagation** algorithm is used to recover the full data block.</span>

- <span style="font-size:0.8em">This method enables a better read/write cost tradeoff.</span>

:::
 


![](assets/large_block.PNG){width=600px height=190px fig-align="center"}

-->


---

## Coding Strategies - Summary

- The following table summarizes the role of each type of code in handling **erasure** and **substitution** errors:

<br>

::: center
| Code Type        | Erasure Errors | Substitution Errors |
|------------------|----------------|----------------------|
| Inner Code       | ✗              | ✓                    |
| Outer Code       | ✓              | ✗                    |
| Large Block Code | ✓              | ✓                    |
:::

---

## $c_r$ vs $c_w$: Simulation Bounds (Separated vs. Large Block)

<div style="font-size:0.6em; line-height: 1.0;">

![](assets/Graph_2.PNG){width=700px height=400px fig-align="center"}

- The parameters used in the simulation are: $n = 1000$, $L = 256$.
</div>


::: {.center}
# 5 Low Density Parity Check (LDPC) {data-name="LDPC"}
:::
## LDPC Codes

<div style="font-size:0.9em; line-height: 1.4;">

- LDPC (Low-Density Parity-Check) codes are a powerful class of linear error-correcting codes.
- First introduced by Gallager in the 1960s, but gained widespread adoption in the 1990s with improved decoding algorithms.
- They are now used in modern systems such as Wi-Fi, 5G, satellite communication, flash storage, and digital broadcasting.
- LDPC codes achieve performance close to the Shannon limit, making them highly efficient for reliable communication.
- They support fast and scalable decoding using iterative message-passing algorithms.

</div>

## LDPC Motivation

<div style="font-size:1.0em; line-height: 1.5;">


Assume we have the following data to be transmitted:

::: {.big-math}
$$
1\hspace{1mm}0\hspace{1mm}0\hspace{1mm}1\hspace{1mm}1\hspace{1mm}0
$$
:::

and after transmitting the message, it was received as:

::: {.big-math}
$$
1\hspace{1mm}?\hspace{1mm}0\hspace{1mm}1\hspace{1mm}1\hspace{1mm}0
$$
:::


</span>

What can help us recover the erasured bit?</span>

</div>

---

## Pairity Check

<div style="font-size:0.9em; line-height: 1.2;">

We can enhance our data by adding a parity check bit:

<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 0 0 1 1 0 <span class="fragment" style="color:red;">1</span>
</div>

<br><br>

<span class="fragment">
Suppose we later receive the following (with one bit erased):
<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 ? 0 1 1 0 <span style="color:red;">1</span>
</div>
</span>

<br>

<span class="fragment">
To recover the missing bit, we compute the parity of the known bits and compare it to the parity check bit. This allows us to deduce the value of the erased bit.
</span>

</div>


---

## What happens if more than 1 erasure?

<div style="font-size:0.8em; line-height: 1.2;">

Assume with have the same data as before with the pairity check bit:

<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 0 0 1 1 0 <span style="color:red;">1</span>
</div> <br>

And now, after transmitting this data, the recieved data is:<br><br>
<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 ? 0 1 ? 0 <span style="color:red;">1</span>
</div> <br>
Can our pairity check bit recover both erasured bits?

</div>

---

## Naive Idea

<div style="font-size:0.6em; line-height: 1.2;">

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.

![](assets/naive1.PNG){width=500px height=240px fig-align="center"}

</div>

---

## Naive Idea

<div style="font-size:0.6em; line-height: 1.2;">
We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.


![](assets/naive2.PNG){width=500px height=240px fig-align="center"}

</div>
---

## Naive Idea

<div style="font-size:0.6em; line-height: 1.2;">

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.

![](assets/naive3.PNG){width=500px height=240px fig-align="center"}

</div>


---

## Naive Idea

<div style="font-size:0.6em; line-height: 1.2;">

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.

![](assets/naive4.PNG){width=500px height=240px fig-align="center"}

</div>

---

## Naive Idea

<div style="font-size:0.6em; line-height: 1.2;">

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.

![](assets/naive5.PNG){width=500px height=240px fig-align="center"}

<span class="fragment"> So now, when the same erasures as before will occur:
<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 ? 0 1 ? 0 <span style="color:red;">1 0</span>
</div><br>
We will be able to recover the erasured bits.
<br><br>
</span>
<span class="fragment">
Are we done? 
</span>

</div>

---

## Possible Solution
### Overlapping Pairity Checks

<div style="font-size:0.7em; line-height: 1.2;">

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa1.PNG){width=700px height=300px}

</div>



---

## Possible Solution
### Overlapping Pairity Checks

<div style="font-size:0.7em; line-height: 1.2;">

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa2.PNG){width=700px height=300px}

</div>


## Possible Solution
### Overlapping Pairity Checks

<div style="font-size:0.7em; line-height: 1.2;">

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa3.PNG){width=700px height=300px}

</div>

---

## Possible Solution
### Overlapping Pairity Checks

<div style="font-size:0.7em; line-height: 1.2;">

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa4.PNG){width=700px height=300px}

</div>

---

## Possible Solution
### Overlapping Pairity Checks

<div style="font-size:0.7em; line-height: 1.2;">

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa5.PNG){width=700px height=300px}

</div>

---

## Possible Solution
### Overlapping Pairity Checks

<div style="font-size:0.7em; line-height: 1.2;">

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa6.PNG){width=700px height=300px}

</div>

---

## Possible Solution
### Overlapping Pairity Checks

<div style="font-size:0.7em; line-height: 1.2;">

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa7.PNG){width=670px height=290px}


- With this 4-bit block pairity checks, we can correct any 2 erasures in the block.

</div>
---

## Possible Solution
### Parity Check Bits Protection

<div style="font-size:0.6em; line-height: 1.2;">

- Consider the previous example:


  <div style="margin-left: 250px; margin-top: -10px; margin-bottom: 5px; text-align: center;">
    <img src="assets/ldpc_exa7.PNG" width="400px" height="150px" style="display: inline-block;">
  </div>

- Assume that the erasures are as follows:

  <div style="margin-left: 250px; margin-top: -10px; margin-bottom: 5px; text-align: center;">
    <img src="assets/pairity_er1.PNG" width="400px" height="160px" style="display: inline-block;">
  </div>

- Can the erasured data bit be recovered?

</div>

---

## Possible Solution
### Parity Check Bits Protection

<div style="font-size:0.8em; line-height: 1.2;">

We can embed **parity-check protection bits** within the existing parity-check structure:

<div style="margin-left: 100px; margin-top: -10px; margin-bottom: 5px; text-align: center;">
  <img src="assets/pairity_protect.PNG" width="800px" height="320px" style="display: inline-block;">
</div>

This allows us to protect and recover even the **parity-check bits themselves**.

</div>


---

## Possible Solution  
### Pairity Chack Implementation

<div style="font-size:0.7em; line-height: 1.2;">

- What would happen if we apply the 4-bit strategy repeatedly over consecutive 4-bit blocks in a longer message?

![](assets/4-bit_ex.PNG){width=500px height=120px fig-align="center"}

- And what if we increase the overlap size together with the amount of pairity checks per overlap — how does that impact performance and error correction?

![](assets/16-bit_ex.PNG){width=600px height=110px fig-align="center"}

</div>


---


## Density and the Role of Overlap

- Therefore, our goal is to design a parity-based error correction code that:
  - Has **low write cost**,  
  - Enables **fast and efficient decoding**,  
  - **recovers erasures effectively**.

- In the 1960s, Robert Gallager introduced a groundbreaking approach for this challenge — now known as LDPC codes (Low-Density Parity-Check codes).



---

## Density and the Role of Overlap

<div style="font-size:0.7em; line-height: 1.2;">
- Gallager showed how to **balance two competing goals**:
  - Reducing the code's redundancy and **High recovery power** (which favors larger overlapping neighborhoods),
  - With **low decoding complexity** (which favors smaller overlaps).

- He introduced the concept of **density**, which is a measure of how many bits participate in each parity check:
  - **Higher density** means more overlap (lower redundancy),
  - **Lower density** means less overlap (faster decoding).

</div>

---

## Low Density

::: {.incremental}

- Gallager showed that even **sparse** parity-checks can achieve strong error correction, by using efficient **decoding algorithms** like belief propagation.

- Sparsity reduces the decoding complexity while still maintaining performance close to Shannon's limit.

- Gallager also showed that **randomly generated** pairity checks perform well with high probability.

- Randomness avoids problematic patterns in the code structure.
:::


## LDPC Theory
### Tanner Graphs

<div style="font-size:0.7em; line-height: 1.2;">

- A **Tanner graph** is a **bipartite graph** with:
  - **Variable nodes**: one for each bit.
  - **Check nodes**: one for each parity-check.
- An edge connects variable node $v_j$ and check node $c_i$ if and only if $v_j$ participates in $c_i$.

Let's take a look at a familiar Tanner graph:

![](assets/general_tanner.PNG){width=630px height=240px fig-align="center"}

</div>

---


## LDPC Theory
### Parity Check Equations

![](assets/general_tanner.PNG){width=580px height=230px fig-align="center"}

<div style="font-size:0.7em; line-height: 1.3;">
This Tanner graph defines 3 parity check equations:
</div>

<div style="font-size:0.7em; line-height: 1.3;">
$$
\begin{aligned}
& c_1 \oplus c_2 \oplus c_3 \oplus c_6 \oplus c_7 = 0 \\
& c_1 \oplus c_2 \oplus c_3 \oplus c_5 = 0 \\
& c_1 \oplus c_3 \oplus c_4 \oplus c_5 = 0
\end{aligned}
$$


</div>

---

## LDPC Theory
### Matrix Formulation

<div style="font-size:0.6em; line-height: 1.3;">
$$
\begin{aligned}
& c_1 \oplus c_2 \oplus c_3 \oplus c_6 \oplus c_7 = 0 \\
& c_1 \oplus c_2 \oplus c_3 \oplus c_5 = 0 \\
& c_1 \oplus c_3 \oplus c_4 \oplus c_5 = 0
\end{aligned}
$$

</div>

<div style="font-size:0.7em; line-height: 1.3;">
Codeword constraints are often written in matrix form. The above constraints become:
</div>

<div style="font-size:0.6em; line-height: 1.3;">
$$
\underbrace{
\begin{bmatrix}
1 & 1 & 1 & 0 & 0 & 1 & 1 \\
1 & 1 & 1 & 0 & 1 & 0 & 0 \\
1 & 0 & 1 & 1 & 1 & 0 & 0
\end{bmatrix}
}_H
\cdot
\begin{bmatrix}
c_1 \\
c_2 \\
c_3 \\
c_4 \\
c_5 \\
c_6 \\
c_7
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0 \\
0
\end{bmatrix}
$$
</div>



## LDPC Theory
### Pairity Check Matrix

<div style="font-size:0.7em; line-height: 1.2;">

- As seen in the previous slide, LDPC code can be presented as a parity-check matrix $H$. <br><br>

- The matrix is composed of $n$ columns and $k$ rows.<br><br>

- Each column represents a bit (pairity bit or regular bit).<br><br>

- Each row represents a parity-check equation over some bits.<br><br>

- Each row has only a few non-zero entries, which mean that the matrix is sparse and the pairity checks have low density.

</div>

---

## LDPC Theory
### Pairity Check Matrix - Codewords

<div style="font-size:0.8em; line-height: 1.2;">

- A codeword is a vector of length $n$ that satisfies all pairity checks.

- As demonstrated, all pairity checks are satisfied if multiplying $H$ with a vector $v$ is 0.

- Therefore, every codeword $c$ must satisfy $Hc=0$.

</div>

---

## LDPC Theory
### Encoding Algorithm

<div style="font-size:0.6em; line-height: 1.2;">
- We’ve seen how the **parity-check matrix** $H$ determines whether a vector $\bar{c}$ is a valid codeword by checking:
  
  $$
  H\bar{c} = 0
  $$

- Our next goal is **to construct such valid codewords** from raw data. That is, we want to define an **encoding matrix** $G$ such that:

  $$
  \bar{c} = G\bar{d}
  $$

  where $\bar{d}$ is the data vector.<br><br>

- This implies that the columns of $G$ must be orthogonal to the rows of $H$.<br><br>

- There exists a well-defined mathematical procedure to construct such a matrix $G$ from $H$.

</div>





<!--
- A LDPC code can be presented as a parity-check matrix $H$.

- The matrix is composed of $n$ columns and $k$ rows, where each column represents a bit (pairity bit or regular bit), and each row represents a parity-check equation over some bits.

- Each row has only a few non-zero entries, which mean that the pairity checks have low Density.

- A vector $\bar{v}\in \{0,1\}^{nX1}$ is a codeward if $Hv=\bar{0}$.

### Example

Let $n = 8$, and $k = 3$. Then one possible $H$ matrix:

$$
H =
\begin{bmatrix}
1 & 1 & 0 & 1 & 0 & 0 & 0 & 0\\
0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 & 1 & 1 & 0
\end{bmatrix}
$$

---

## Example - cont. {.smaller}

We use the same parity-check matrix:

$$
H =
\begin{bmatrix}
1 & 1 & 0 & 1 & 0 & 0 & 0 & 0\\
0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 & 1 & 1 & 0
\end{bmatrix}
$$

Let’s check if the vector

$$
c =
\begin{bmatrix}
1 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 1 \\ 0
\end{bmatrix}
$$

is a codeword by computing $Hc$:

---

### Computation {.smaller}

We compute:

<div style="font-size:0.6em;">

$$
Hc =
\begin{bmatrix}
1 & 1 & 0 & 1 & 0 & 0 & 0 & 0\\
0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 1 & 1 & 1 & 0
\end{bmatrix}
\begin{bmatrix}
1 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 1 \\ 0
\end{bmatrix}
$$

</div>

$$
\Downarrow
$$

<div style="font-size:0.8em;">
\begin{align*}
1\cdot x_1 + 0\cdot x_2 + 0\cdot x_4 &= 1 + 0 + 0 = 1 \\
0\cdot x_2 + 1\cdot x_3 + 0\cdot x_8 &= 1 + 0 + 0 = 1 \\
1\cdot x_1 + 0\cdot x_5 + 1\cdot x_6 + 1\cdot x_7 &= 1 + 0 + 1 + 1 = 1
\end{align*}


</div>

<br>
So this is **not** a codeword, because $Hc \neq 0$.

---

## LDPC Encoding

- We introduced LDPC codes using a **parity-check matrix** $H$, where a vector $\bar{v}$ is a **codeword** if it satisfies:
$$
H\bar{v} = 0.
$$

- Our goal now is to generate such codewords from raw data — in other words, to define an **encoding matrix** $G$ such that:
$$
\bar{v} = G\bar{d}
$$
for some data vector $\bar{d}$, and the result $\bar{v}$ is guaranteed to satisfy $H\bar{v} = 0$.

- Since any encoded vector $\bar{v}$ is a **linear combination of the columns of $G$**, This means that the set of all possible codewords is exactly the **column space** of $G$.
  
- Therefore, it must hold $HG=\{0\}$

---

- A common way to construct such a $G$ is to first put $H$ into **systematic form**:
$$
H = \left[ P \,\middle|\, I_m \right]
$$
where $P$ is a matrix of size $m \times k$ and $I_m$ is the identity matrix of size $m = n - k$.

- Then, the generator matrix $G$ is defined as:
$$
G = \left[ I_k \,\middle|\, P^\top \right]
$$
where $I_k$ is the identity matrix of size $k$ and $P^\top$ is the transpose of $P$.


## LDPC - Decoding Introduction

- Checking if a vector is a valid codeword via matrix multiplication with $H$ can be computationally expensive.

<br>

- However, **$H$ is sparse** in LDPC codes.

<br>

- We can exploit this sparsity by representing $H$ as a **graph structure**, enabling more efficient decoding.

<br>

- We present the concept of tanner graphs.

---

## Tanner Graphs: Definition

- A **Tanner graph** is a **bipartite graph** with:
  - **Variable nodes**: one for each bit (column of $H$)
  - **Check nodes**: one for each parity-check (row of $H$)
- An edge connects variable node $v_j$ and check node $c_i$ if and only if $H_{i,j} = 1$.

![](assets/tanner.PNG){width=900px height=400px fig-align="center"}

---

## Tanner Graphs : Example

![](assets/tanner.PNG){width=900px height=400px fig-align="center"}


<div class="fragment">
The corresponding matrix is:
$$
H =
\begin{bmatrix}
1 & 1 & 0 & 0 & 1 & 0 \\
0 & 0 & 1 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 1
\end{bmatrix}
$$

</div>

---

<!--

## Gallager’s Bit-Flipping Algorihm

- Bit-Flipping is an iterative decoding algorithm, presented by Gallagher. <br> <br>
- Bit-Flipping is used for decoding LDPC codes over a **Binary Symmetric Channel (BSC)**. <br><br>
- The core idea of the algortihm is to flip bits that participate in **many unsatisfied parity checks**.

---

## Algorithm Overview

1. **Input**: Received vector $\bar{r}$ (possibly corrupted), and parity-check matrix $H$.
2. **Check**: For each parity check, compute whether it's satisfied usign Tanner graph.
3. **Count**: For each bit, count how many unsatisfied checks it participates in.
4. **Flip**: Bits involved in "enough" unsatisfied checks are flipped iteratively.
5. **Repeat**: Iterate until all checks are satisfied or a max number of steps is reached.

---

## Example

Let:
$$
H =
\begin{bmatrix}
1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 1 &  \,0
\end{bmatrix}
\quad \bar{r} = [0\ 1\ 0\ 0\ 1\ 0\ 0]
$$

And the Tanner graph:

![](assets/tanner2.PNG){width=800px height=270px fig-align="center"}


---

## Step 1: Compute Syndromes

We check whether the pairity checks are satisfied:
![](assets/tanner3.PNG){width=800px height=400px fig-align="center"}

Almost all checks are unsatisfied! We count how many unsatisfied checks each bit participates in.

---

## Step 2: Flip Bits

![](assets/tanner3.PNG){width=500px height=200px fig-align="center"}

<div style="font-size:0.8em;">

- Bit 1 appears in 1 unsatisfied check  
- Bit 2 appears in 2 unsatisfied checks.
- Bit 3 appears in 1 unsatisfied check. 
- Bit 4 appears in 1 unsatisfied check.
- Bit 5 appears in 3 unsatisfied checks.
- Bit 6 appears in 1 unsatisfied check.
- Bit 7 appears in 1 unsatisfied checks.

Then we flip bit 5.

</div>


---

## Step 2: Flip Bits

![](assets/tanner4.PNG){width=520px height=220px fig-align="center"}

<div style="font-size:0.8em;">
- Bit 1 appears in 1 unsatisfied check  
- Bit 2 appears in 3 unsatisfied checks.
- Bit 3 appears in 2 unsatisfied check. 
- Bit 4 appears in 1 unsatisfied check.
- Bit 5 appears in 1 unsatisfied checks.
- Bit 6 appears in 0 unsatisfied check.
- Bit 7 appears in 0 unsatisfied checks.

Then we flip bit 2.

</div>

---


## Step 2: Flip Bits

![](assets/tanner5.PNG){width=800px height=400px fig-align="center"}

This is Our Codeword!



---

## Summary

- Bit-Flipping is simple and intuitive.  
- Works well with **sparse LDPC matrices**.  
- Not optimal, but useful as a first decoding step or when low complexity is needed.

</span>

-->


## LDPC Decoding: Belief propagation algorithm
- Gallager introduced the idea of iterative, messagepassing decoding of LDPC codes.
- The idea is to iteratively share the results of local node decoding by passing them along edges of the Tanner graph.
- We will demonstrate this decoding method for the BSC

## Message decoding
::: {.r-hstack style="gap:40px; align-items:center; justify-content:center;"}

::: {style="display:flex; flex-direction:column; align-items:center;"}

**Y**

↓

::: {style="background:#6c757d; width:130px; height:60px; color:white; display:flex; align-items:center; justify-content:center; border-radius:8px; font-weight:bold;"}
Decode
:::

↓

**X**

:::

:::


## Probabilistic Decoding (Posterior from Prior)

::: {.r-hstack style="gap:40px; align-items:center; justify-content:center;"}

::: {style="display:flex; flex-direction:column; align-items:center;"}

**Prior** ($P(x=0)=0.7, P(x=1)=0.3$)

↓

::: {style="background:#6c757d; width:130px; height:60px; color:white; display:flex; align-items:center; justify-content:center; border-radius:8px; font-weight:bold;"}
Decode
:::

↓

**Posterior** ($P(x=0 \mid y)=0.9, P(x=1 \mid y)=0.1$)

:::

:::

## Optimality of the Sum–Product Algorithm

:::{.incremental}
  $$\hat x_i=\arg\max_{x_i\in\{0,1\}} P(X_i=x_i \mid \mathbf y)$$
- The code bits $x_i$ are independent  
- The messages passed are independent ⟹ the Tanner graph has no cycles
- **Not** met in practice
:::

## A-priori and LLRs probability {.smaller}

:::{.incremental}
- **Definition**: For a binary variable $x_i$ The **log of posterior ratio** is given by: $$L_\text{posterior}(x_i)=\log\frac{p(x_i=0 \mid y)}{p(x_i=1 \mid y)}$$
- The **log prior ratio** is given by: $$L_\text{prior}(x)=\log\frac{p(x=0)}{p(x=1)}$$
- The log-likelihood ratio (LLR) is given by: $$\tilde L_i(x)=\log\frac{p(y \mid x=0)}{p(y \mid x=1)}$$
- $L(x) > 0$ ⇒ bit more likely 0, $L(x) < 0$ ⇒ bit more likely 1.
:::

## Intuition for the Sum–Product Algorithm

:::{.incremental}
- Iterative message-passing refines $\tilde L_i$ into **a-posteriori** LLRs that incorporate the parity-check constraints.
- $L_\text{posterior}(x_i)=\log\frac{p(x_i=0 \mid y)}{p(x_i=1 \mid y)}$, $L_\text{prior}(x)=\log\frac{p(x=0)}{p(x=1)}$
-  $\tilde L_i(x)=\log\frac{p(y \mid x=0)}{p(y \mid x=1)}$
- Bayes: $$L_\text{posterior}=\tilde{L}_i + L_\text{prior}(x)$$
:::

## *Variable* and *Check* Node Connection Sets

:::{.incremental}
For an LDPC code with parity-check matrix $H\in \mathbb{R}^{m \times n}$
- Let $\mathcal N(i)$ denote the **connection set of variable node** $v_i$ — the **check-node indices** connected to $v_i$  
  $$\mathcal N(i)=\bigl\{\,j : H_{j,i}=1\bigr\}.$$

- Let $\mathcal M(j)$ denote the **connection set of check node** $c_j$ — the **variable-node indices** connected to $c_j$  
  $$\mathcal M(j)=\bigl\{\,i : H_{j,i}=1\bigr\}.$$
:::

## Definition – Variable → Check Messages

:::{.incremental}
- We denote by $L^{[v]}_{i \to j}$ the LLR information **computed by variable node** $v_i$ and sent (along its edge in the Tanner graph) **to check node** $c_j$.
- We denote by $L^{[c]}_{i \leftarrow j}$ the LLR information **computed by check node** $c_j$ and sent (along its edge in the Tanner graph) **to variable node** $v_i$.
:::

## Sum–Product Decoding Algorithm {.smaller}
:::{.incremental}
1. **Init:** For every $i\in\{1,\dots,n\}$ set $L^{[v]}_{i\to j}=\tilde L_i=L(y_i\mid X_i)$ for all $j\in\mathcal N(i)$.
2. **CN update:** For each check node $c_j\;(j\in\{1,\dots,m\})$ compute $$L^{[c]}_{i\leftarrow j}=2\tanh^{-1}\!\Bigl(\prod_{i'\in\mathcal M(j)\setminus\{i\}}\tanh\!\bigl(L^{[v]}_{i'\to j}/2\bigr)\Bigr),\quad\forall i\in\mathcal M(j).$$
3. **VN update:** For each variable node $v_i\;(i\in\{1,\dots,n\})$ compute $$L^{[v]}_{i\to j}=\tilde L_i+\sum_{j'\in\mathcal N(i)\setminus\{j\}}L^{[c]}_{i\leftarrow j'},\quad\forall j\in\mathcal N(i).$$
4. **Total LLR & decision:** $L^{[\text{total}]}_i=\tilde L_i+\sum_{j\in\mathcal N(i)}L^{[c]}_{i\leftarrow j},\qquad \hat x_i=\begin{cases}1,&L^{[\text{total}]}_i<0,\\[4pt]0,&\text{otherwise.}\end{cases}$
Stop if $H\hat{\mathbf x}^T=0$ **or** the iteration limit is reached; otherwise return to **step 2**.
:::

## Example - Channel LLRs for a BSC
$H=\left[\begin{array}{llllll}
1 & 1 & 0 & 1 & 0 & 0 \\
0 & 1 & 1 & 0 & 1 & 0 \\
1 & 0 & 0 & 0 & 1 & 1 \\
0 & 0 & 1 & 1 & 0 & 1
\end{array}\right] \implies$

The following Tanner graph:
<div style="center; position:relative;">
![](assets/tannerex.png){fig-align="center"}
</div>

## Example – Channel LLRs for a BSC {.smaller}

For a BSC with crossover probability $p=0.2$, assume: $$\mathbf x=[0\;0\;1\;0\;1\;1] \qquad \mathbf y=[1\;0\;1\;0\;1\;1]$$

The LLR for bit $i$ is  
$$
\tilde L_i(x)=\log\frac{p(y \mid x=0)}{p(y \mid x=1)}=
\begin{cases}
\log\dfrac{p}{1-p}, & y_i=1,\\[6pt]
\log\dfrac{1-p}{p}, & y_i=0.
\end{cases}
$$

With $\log\dfrac{0.2}{0.8}=-1.3863$ and $\log\dfrac{0.8}{0.2}=+1.3863$ we get  

$$
\mathbf{\tilde L}=[\,-1.3863,\;1.3863,\,-1.3863,\;1.3863,\,-1.3863,\,-1.3863\,].
$$
These values are the initial messages $\tilde L_i$ fed into the decoder.


## LDPC Decoding example
<iframe src="./assets/BP.html" width="1920" height="800" style="border: 1px solid #ccc" frameborder=0></iframe>


::: {.center}
# 6 Encoding/Decoding Schema {data-name="Schema"}
:::

## Encoding a single binary file {auto-animate=true auto-animate-easing="ease-in-out"}

- File size: $192_\text{KB} = 192 \times 1000_\text{B} = 1 536 000_\text{bits}$
- Encoding steps from: 
[https://github.com/shubhamchandak94/LDPC_DNA_storage](https://github.com/shubhamchandak94/LDPC_DNA_storage/blob/master/dna_storage.py)
- KB is decimal (1000 bytes = 1 KB), not binary (1024 bytes = 1 KiB)

::: {.r-hstack style="align-items: center; justify-content: center;"}
::: {data-id="strand1"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand2"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand3"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand4"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand5"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand6"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
:::

<!-- ------------- SLIDE 2 ------------- -->
## Large block LDPC encoding {auto-animate=true auto-animate-easing="ease-in-out"}

- LDPC data-block size (`LDPC_dim=256K`): $256 000_\text{bits}$
- Number of data blocks: $\frac{1 536 000}{256 000} = 6$
- Added parity bits (`LDPC_alpha=0.5`): For each data block, we add $128 000_\text{bits}$ of parity bits. 
- Encoded bits per block: $$256 000_\text{bits} + 128 000_\text{bits} = 384 000_\text{bits}$$

::: {.r-hstack style="align-items:center; justify-content:center;"}

<!-- container keeps the total width at exactly 700 px -->
::: {.r-hstack style="width:700px; gap:0px;"}

<!-- 6 orange data pieces -->
::: {data-id="strand1"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand2"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand3"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand4"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand5"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand6"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  

<!-- 6 blue parity pieces -->
::: {data-id="strand7"  style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand8"  style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand9"  style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand10" style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand11" style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand12" style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  

:::
:::

<!-- ───────────────── SLIDE 3 ───────────────── -->
## Segment and  map to DNA {auto-animate=true auto-animate-easing="ease-in-out"}

- Binary mapping: $00 \to A$, $01 \to C$, $10 \to G$, $11 \to T$
- Bits per oligo (`payload size=84bp`): $84 \times 2_\text{bits} = 168_\text{bits}$
- Number of oligos per block: $$\frac{\text{payload bits}}{\text{bits per oligo}}=\frac{384 000_\text{bits}}{168_\text{bits}} = 2285.71 \approx 2286_\text{oligos}$$
- Total number of oligos: $\text{blocks} \times \text{oligos per block} = 6 \times 2286 = 13716$ (✔)

::: {.r-hstack style="gap:60px;justify-content:center;"}

<!-- ─────────── column 1 ─────────── -->
::: {.r-vstack style="gap:10px;"}
::: {data-id="strand1" style="background:#ff9f1c;width:230px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand2" style="background:#ff9f1c;width:230px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand3" style="background:#ff9f1c;width:230px;height:24px;border-radius:4px;"}
:::
:::

<!-- ─────────── column 2 ─────────── -->
::: {.r-vstack style="gap:10px;"}
::: {data-id="strand7"  style="background:#1982c4;width:165px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand8"  style="background:#1982c4;width:165px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand9"  style="background:#1982c4;width:165px;height:24px;border-radius:4px;"}
:::
:::
:::


## Encoded oligo structure {auto-animate=true auto-animate-easing="ease-in-out"}
| Segment                             | bp         |
| ----------------------------------- | ---------- |
| BCH-protected index                 | $14_\text{bit} / 2 = 7_\text{bp}$      |
| BCH redundancy      | $12_\text{bit} / 2 = 6_\text{bp}$       |
| Sync marker (`AGT`)            | $3_\text{bp}$       |
| **Payload** | $84_\text{bp}$      |
| Primers                   | $2 \times 25_\text{bp}$       |
| **Total**                            | **$150_\text{bp}$**  |

::: {.r-vstack style="align-items:center; justify-content:center; gap:6px;"}

<!-- ─────────── 1 ►  COLOURED BAR  ─────────── -->
::: {.r-hstack style="align-items:center; justify-content:center;"}

::: {data-id="primerL"
     style="background:#6c757d;  width:100px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;
            border-top-left-radius:6px; border-bottom-left-radius:6px;"}
FW
:::

::: {data-id="indexECC"
     style="background:#d90429;  width:200px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;"}
Index + ECC
:::

::: {data-id="payloadL"
     style="background:linear-gradient(135deg,
                   #ff9f1c 0%,   /* orange in the upper-left corner   */
                   #1982c4 100%);/* blue in the lower-right corner    */
       width:200px; height:60px;
       color:#fff; font-weight:bold;
       display:flex; align-items:center; justify-content:center;"
}
Payload
:::

::: {data-id="sync"
     style="background:#38b000;  width:100px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;"}
AGT
:::

::: {data-id="payloadR"
     style="background:linear-gradient(135deg,
                   #ff9f1c 0%,   /* orange in the upper-left corner   */
                   #1982c4 100%);/* blue in the lower-right corner    */
       width:200px; height:60px;
       color:#fff; font-weight:bold;
       display:flex; align-items:center; justify-content:center;"
}
Payload
:::

::: {data-id="primerR"
     style="background:#6c757d;  width:100px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;
            border-top-right-radius:6px; border-bottom-right-radius:6px;"}
REV
:::
:::

<!-- ─────────── 2 ►  bp LABEL ROW  ─────────── -->
::: {.r-hstack style="gap:0px; font-size:0.8em; font-weight:bold; color:#000;"}

::: {data-id="primerL_lbl"   style="width:100px; text-align:center;"}
25 bp
:::

::: {data-id="indexECC_lbl"  style="width:200px; text-align:center;"}
6+7 bp
:::

::: {data-id="payloadL_lbl"  style="width:200px; text-align:center;"}
42 bp
:::

::: {data-id="sync_lbl"      style="width:100px; text-align:center;"}
3 bp
:::

::: {data-id="payloadR_lbl"  style="width:200px; text-align:center;"}
42 bp
:::

::: {data-id="primerR_lbl"   style="width:100px; text-align:center;"}
25 bp
:::
:::
:::

## Encoding schema overview

::: {.r-stack style="height:60vh; display:flex; flex-direction:column; justify-content:center; align-items:center;"}
![](assets/encoding.png)
:::

## Challenges data storage in DNA
| **Error Type**              | **Solution**                                                               |
|----------------------------|---------------------------------|
| 1. Unordered reads           | • Addressing index + BCH codes                                            |
| 2. Insertion/deletion errors | • Convert to substitution/erasure <br> • Sync markers<br> • MSA (via indexed clusters) |
| 3. Substitution errors       | • LDPC codes                                                                 |

## Synchronization Marker {.smaller}

- **Goal:** Convert insertion/deletion errors into erasures for LDPC decoder
- If a read has unexpected length (due to indels), we try to **recover the marker** using Multiple Sequence Alignment (MSA).
- If the marker is located:
  - We **split** the read at the marker.
  - Retain only the expected-length portion.
  - Mark the rest as **erasures**.
- In simulations: 10% improvement in the reading cost while having little impact (2-3%) on the writing cost.

::: {.r-vstack style="align-items:center; justify-content:center; gap:6px;"}

<!-- ─────────── 1 ►  COLOURED BAR  ─────────── -->
::: {.r-hstack style="align-items:center; justify-content:center;"}

::: {data-id="payloadL"
     style="background:#ff9f1c;  width:200px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;"}
Payload
:::

::: {data-id="sync"
     style="background:#38b000;  width:100px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;"}
AGT
:::

::: {data-id="payloadR"
     style="background:#ff9f1c;  width:200px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;"}
Payload
:::
:::
:::


## Multiple-Sequence Alignment (MSA) {.smaller}

- Cluster **per index bucket** (≈ 3–10 reads, 150 bp) → tiny, fast alignments.  
- **Lines up indels** → gaps become **erasures (?)** instead of bad bits.  
- Outputs **base counts**
- If consensus length off, use the in-strand **“AGT” marker** to trim half and mark the rest erasures.  
- **Result:** LDPC sees a large code-word with *substitutions + explicit erasures* it can correct simultaneously.

<div style="center; position:relative;">
![](assets/MSA.png){fig-align="center" width="35%"}
</div>
    
## Is MSA a Bottleneck? {visibility="hidden"}

> With thousands of strands, won’t multiple-sequence alignment blow up?

Theoretical cost (MUSCLE algorithm)

For $N$ sequences of length $L$ bases (Egdar, 2004):
$$
\mathcal{O}\!\left(N^{2}L\;+\;N L^{2}\right)
$$
<div style="font-size:0.5em;">
Edgar, R. C. (2004). *MUSCLE: multiple sequence alignment with high accuracy and high throughput.* *Nucleic Acids Research, 32*(5), 1792-1797. https://doi.org/10.1093/nar/gkh340
</div>

## Is MSA a Bottleneck? {visibility="hidden"}

$$
\mathcal{O}\!\left(N^{2}L\;+\;N L^{2}\right)
$$

| Parameter | Typical value | Work per MSA call |
|-----------|--------------:|-------------------|
| Coverage per oligo (*N*) | 3 – 12 | **tiny N** |
| Oligo length (*L*)       | 100 bp | **tiny L** |
| Calls per decode         | 35-124k | linear growth |

→ LDPC decoding, not MSA, dominates CPU time.


::: {.center}
## Decode Schema
:::

## The Decode pipeline {.smaller}

1. **Primer trim & orientation** → raw reads  
2. **Index + BCH decode**  
   * correct up to 2–3 bit errors  
   * single-indel heuristic recovers extra reads  
3. **Per-index MSA (+ sync marker)**  
   * aligns indels, outputs counts → LLRs  
   * truncated halves → erasures  
4. **Large-block LDPC decode**  
   * fixes remaining substitutions **and** erasures (missing strands) in one shot

## Decoding – how the counts become LLRs {.smaller}

After per-index MSA we have counts $k_0,k_1$ (gaps discarded).

$$\boxed{\,\text{LLR}(k_0,k_1)\;=\ln\frac{P((k_0,k_1)|0)}{P((k_0,k_1)|1)}=\;(k_0-k_1)\,
        \ln\!\Bigl(\tfrac{1-\varepsilon}{\varepsilon}\Bigr)}$$

* $k_0,k_1$ = how many times column voted “0” or “1”  
* $\varepsilon$ = post-MSA substitution rate (paper uses 4 %)  
* Positive → bit likely **0**, negative → bit likely **1**, near 0 → **erasure**

These soft LLRs seed the LDPC **belief-propagation** decoder instead of hard 0/1 decisions.


::: {.center}
# 7 Experimental results {data-name="Results"}
:::

## Writing Cost $c_w$: what we pay to synthesize

$$
c_w \;=\;
\frac{\text{bases synthesized}}
     {\text{file size (bits)}}
$$

- **Bases synthesized** = $\text{# oligos} \times (\text{oligo length} - \text{primers})$  

*Unit: bases / information bit — lower is cheaper to write.*


## Example – Experiment 1 {visibility="hidden"}

| Quantity | Value |
|----------|------:|
| \# oligonucleotides | 11 710 |
| Oligo length | 150 bp |
| Primers | 50 bp |
| Synthesized payload / oligo | $150-50 = 100$ bp |
| File size | 160 000 B = 1 280 000 bits |

$$
\text{bases synthesized} =
11\,710 \times 100 = 1.171\times10^{6}
$$

$$
c_w =
\frac{1.171\times10^{6}}
     {1.28\times10^{6}}
   = 0.91 \;\text{bases/bit}
$$

(✔) Matches Table 1

## Reading Cost $c_r$: what we pay to read {.smaller}

1. **Physical coverage** – lab metric
$$
\text{Raw coverage} =
\frac{\text{total aligned reads}}
     {\text{distinct oligonucleotides}}
$$

2. **Reading cost $c_r$** – information metric
$$
c_r =
\frac{\text{bases sequenced for a guaranteed decode}}
     {\text{file size (bits)}}
$$
*Unit: bases / bit*

How do we find the numerator: *Empirical threshold:* randomly subsample the read pool until decoding works **20 / 20 times** → that read count is “min reads for decode”.


## Concrete example – Experiment 1 {visibility="hidden"}

| quantity | value |
|----------|------:|
| Min. reads (20/20 decode) | 35 000 |
| Oligo length | 150 bp |
| Primers | 2 × 25 bp |
| Payload / read | 150 – 50 = 100 bp |
| File size | 160 000 B = 1 280 000 bits |

$$
\text{bases read}=35\,000\times100=3.5\times10^{6}
$$

$$
c_r = \frac{3.5\times10^{6}}{1.28\times10^{6}}
     = 2.73\ \text{bases/bit}
$$

(✔) Matches Table 1.

## Write/read costs (Fig. 8 / Table 1) {.smaller}

<!-- Table block -->
<div style="position:relative;">

| Exp. No. | LDPC Redundancy | File Size | No. of Oligonucleotides | Normalized Coverage Variance | Writing Cost (bases/bit) | Reading Cost (bases/bit) |
|----------|------------------|-----------|--------------------------|-------------------------------|---------------------------|---------------------------|
| 1        | 50%              | 160 KB    | 11,710                   | 1.97                          | 0.91                      | 2.73                      |
| 2        | 10%              | 224 KB    | 12,026                   | 1.57                          | 0.67                      | 3.82                      |
| 3        | 50%              | 192 KB    | 13,716                   | 3.36                          | 0.89                      | 3.45                      |
| 4        | 30%              | 192 KB    | 11,892                   | 3.53                          | 0.78                      | 4.46                      |
| 5        | 10%              | 192 KB    | 10,062                   | 3.19                          | 0.66                      | 8.11                      |

<!-- Floating figure -->
<div class="fragment"
     style="position:absolute; top:50px; right:-60px; width:790px; z-index:10;">
  <img src="assets/figure8.png" style="width:150%;">
</div>

</div>



## The ugly: experiment 9 {.smaller}

| Exp. No. | LDPC Redundancy | BCH Parameter | Index Length (bp) | Sync Marker | Sync Pos. | File Type | File Size (bytes) | No. of Oligos | Writing Cost (bases/bit) |
|----------|-----------------|---------------|-------------------|-------------|-----------|-----------|-------------------|---------------|--------------------------|
| 9        | 0.1             | 1             | 10                | None        | –         | Text      | 286,432           | 14,094        | 0.62                     |

For **Experiment 9**, which utilized the weakest error correction settings (0.1 LDPC Redundancy), a definitive "minimum coverage for decoding" is **not reported**.

*   The decoding process **did not succeed for 20 out of 20 trials**, even when very high reading costs were applied
*   While decoding *did* succeed for some fraction of trials at reading costs above 10 bases/bit, it did not meet the consistent success criteria (20/20 trials) required for reporting a minimum value

## Error profile (Fig. 9)
::: {.r-stack style="height:80vh; display:flex; flex-direction:column; justify-content:center; align-items:center;"}
![](assets/figure9.png)
:::

## Coverage profile (Fig. 10)
::: {.r-stack style="height:80vh; display:flex; flex-direction:column; justify-content:center; align-items:center;"}
![](assets/figure10.png)
:::

<!--
## Indel correction heuristics (Table. 4)

<div style="center; position:relative;">
![](assets/table4.png){fig-align="center" width="80%"}
</div>

- Attempting to correct single indel with a BCH code
- This step is able to correct 5-10% additional indexes.

## Probability of dec. failure (Fig. 11)
::: {.r-stack style="height:80vh; display:flex; flex-direction:column; justify-content:center; align-items:center;"}
![](assets/figure11.png)
:::

-->

## Stress testing

* Simulated **6% total error** (2% sub / 2% del / 2% ins)
* Added **15% random reads** (unaligned noise)
* **224 KB** file, **50% LDPC**, BCH (3-error correction)
* Increased LDPC decoder threshold to **10%**
* **Write cost**: 1.07 bases/bit
* **Decoding succeeded** at **10.5 bases/bit read cost**


::: {.center}
# 8 Concluding Remarks {data-name="Conclusions"}
:::

## Conclusions

* Achieves **better read/write cost tradeoff** than prior work
* Combines **LDPC codes** with **heuristics** for indel correction
* Insights may help improve **bioinformatics tools** and **error models**

## Future Work

* Use **channel-optimized LDPC** or **marker codes**
* Extend to **nanopore sequencing** (high indel rates)
* Improve **index error correction efficiency**

## Limitations
* **Random access** No range queries or selective decoding
* **Counting on heuristics** for indel correction, which may not generalize well
* **Error model** Assumes independent errors, which may not hold in practice
* **Relying on simulations** for performance evaluation, which may not fully capture real-world complexities


## Crafting the Presentation: Tools
- [`Quarto`](https://quarto.org/): markdown-based authoring system that supports multiple output formats.
- [`revealjs`](https://revealjs.com/): a framework for creating interactive presentations using HTML and JavaScript.
  - [`simplemenu`](https://github.com/Martinomagnifico/quarto-simplemenu): a plugin to create a menu bar that allows us to navigate through the presentation.

## In the words of Robert Gallager

::: {.columns}

::: {.column width="60%"}

*LDPC was an interesting midpoint, cute and interesting to theoriticanats but 35 years before the technological feasibility. That's the way research is and the way it should be. Hard research problems take years to solve and should not be overly dependant on details of current technological capabilities ... Applications resolve when both are ready.*

:::

::: {.column width="40%"}

<div style="center; position:relative;">
![](assets/galager.jpg){fig-align="center" width="50%"}
</div>
[Dr. Robert Gallager, 2019](https://youtu.be/RWUxtGh-guY?si=-pmnOEU7dOzqA7qa&t=761)
:::

:::
