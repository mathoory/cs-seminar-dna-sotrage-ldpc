---
title: "Seminar on DNA Data Storage"
subtitle: "Improved read/write cost tradeoff in DNA-based data storage using LDPC codes<br><br>Shubham Chandak, Kedar Tatwawadi, Billy Lau, Jay Mardia, Matthew Kubit, Joachim Neu, Peter Griffin, Mary Wootters, Tsachy Weissman, Hanlee J<br>(2019)<br><br>Presented by: Atar Ron and Mattan Hoory"
format:
  revealjs: 
    title-slide-attributes:
      data-state: "hide-menubar"
    slide-number: true
    preview-links: auto
    css: style.css
    logo: assets/CS_LOGO.jpg
    footer: 'DNA Data Storage - 02360801 - Spring 2025'
    toc: true
    simplemenu:
        flat: true
        barhtml:
            header: "<div class='menubar'><ul class='menu'></ul><div>"
        scale: 0.67

revealjs-plugins:
  - simplemenu
---

## Introduction {data-name="Intro"}

In DNA-based data storage, there are two critical challenges:

::: {.incremental}

- **Write cost** — how much synthetic DNA we need to store one bit of information  
- **Read cost** — how many sequencing reads are needed to reliably recover that bit


<span class= "fragment"> These two costs are closely related, and thus, one of the most important problems in DNA data storage is finding an effective tradeoff between them. </span>

<div style="text-align: center;">
  <img src="assets/write_read_proc.PNG" width="600px" height="300px" class="fragment">
</div>

:::

---

## Discussion Questions

::: {.incremental}

- Why , in your opinion, are the cost of write and cost of read closely related? how does the write cost affect the read cost?
&nbsp;  
- What are the main factors that affect the cost of write?  
&nbsp;  
- What are the main factors that affect the cost of read?

:::


---

## Main Goals

::: {.incremental}

Since the cost of write and cost of read are strongly correlated, this paper aims to:


- Establish a theoretical lower bound on the tradeoff between write cost and read cost.
- Design and evaluate a practical coding scheme (based on LDPC codes) that achieves a better tradeoff than previous methods.
- Validate the performance of the scheme through both real experiments (DNA synthesis and sequencing) and simulations.

:::

# Theoretical Bounds {data-name="Theoretical Bounds"}

## Cost of Read and Write 

* **Cost Of Write**
  Average number of encoded bits synthesized per information bit:

  $$
  c_w = \frac{|\text{SYN}|}{|\text{DATA}|}
  $$

* **Cost of Read**
  Average number of bits read per information bit:

  $$
  c_r = \frac{|\text{READ}|}{|\text{DATA}|}
  $$

## Coverage

* **Coverage**- Average number of bits read per synthesized bit:

  $$
  \text{Coverage} = \frac{|\text{READ}|}{|\text{SYN}|}
  = \frac{\frac{|\text{READ}|}{|\text{DATA}|}}{\frac{|\text{SYN}|}{|\text{DATA}|}}
  = \frac{c_r}{c_w}
  $$


::: {.incremental}
- Coverage has been widely used in prior work to estimate the efficiency of read/write tradeoffs in DNA-based storage systems. 
<br> </span>

- It measures how many sequencing reads are made per synthesized bit — but is that the best way to evaluate system performance?
<br></span>

:::
---

## Is Coverage a Good Metric?

<span style="font-size:0.9em">We consider the following example that demonstrates why coverage can be misleading.</span><br>

### Example
<span style="font-size:0.9em">Suppose we compare two storage systems with the following properties:</span>

| System        | $c_w$ | $c_r$ | $\text{coverage} = c_r/c_w$ |
|----------------|------------|----------------|------------------------|
| A  | 4                   | 12                 | 3                   |
| B  | 2                   | 10                 | 5                   |

* <span style="font-size:0.9em">**Note:** In this example, we assume that both the read cost and the coverage values were measured after decoding all the sampled strands.</span>



## Example - cont. 

| System        | $c_w$ | $c_r$ | $\text{coverage} = c_r/c_w$ |
|----------------|------------|----------------|------------------------|
| A  | 4                   | 12                 | 3                   |
| B  | 2                   | 10                 | 5                   |

* <span style="font-size:0.9em">At first glance, System A has better (lower) coverage. Therefore, when evaluating the systems based on coverage only, we will prefer System A. </span>  


* <span style="font-size:0.9em"> But, it is easy to see that System B reads fewer total bits per information bit (10 vs. 12), and also synthisizes significantly less DNA. So despite having higher coverage, System B is clearly more efficient overall. </span>


---

## Example - cont. 
<br>
**Conclusion:** <span style="font-size:0.9em">This example illustrates that relying solely on coverage can be misleading, particularly when comparing systems with varying write costs. A more meaningful comparison is to evaluate the actual read and write costs per information bit.</span>  
<br>

**Important Note:** <span style="font-size:0.9em">We can also define coverage at the strand level: the average number of times a strand is observed in the sampled reads. We'll refer back to this concept later on.</span>


---

## Model Notations

- $n$ – Number of strands.  
- $L$ – Strand length.  
- $c_{w}$ – Cost of write.  
- $c_{r}$ – Cost of read.  
- $|\text{SYN}|$ – Total number of bases synthesized.  
- $|\text{READ}|$ – Total number of bases read.
- $\epsilon$ - Substitution error rate.


## Model Definition {.nostretch}

::: {.fragment data-fragment-index="1"}
- <span style="font-size:0.8em">The storage system encodes $n$ information strands, each of length $L$. </span>
:::

::: {.fragment data-fragment-index="3"}
- <span style="font-size:0.8em">To store the data with write cost $c_w$, the encoder synthesizes $n \cdot c_w$ strands of length $L$.</span>
:::

::: {.fragment data-fragment-index="5"}
- <span style="font-size:0.8em"> To retrieve the data with read cost $c_r$, the decoder samples $n \cdot c_r$ strands of length $L$. </span>
:::

<!-- Images side-by-side and controlled with fragments -->
<div style="margin-bottom: 200px;">
  <img src="assets/model_process1.PNG" width="200" class="fragment" data-fragment-index="2">
  <img src="assets/model_process2.PNG" width="350" class="fragment" data-fragment-index="4">
  <img src="assets/model_process3.PNG" width="350" class="fragment" data-fragment-index="6">
</div>


---


## Model Definition - cont.

<!-- Final bullet -->
- <span style="font-size:0.7em"> It is assumed, for simplicity, that the decoder has access to the index of each strand, and that deletion and insertion errors are ignored. We later explain how the authors overcame this in practice. </span>

- <span style="font-size:0.7em"> The reads are subject to:</span>

  - <span style="font-size:0.7em"> **Substitution errors** — each sampled strand is passed through a Binary Symmetric Channel (BSC) with bit-flip probability $\epsilon$. This simulates sequencing errors.</span>
  - <span style="font-size:0.7em"> **Sampling variability** — the number of times each strand is sampled is random (Poisson-distributed).</span>

![](assets/model_process4.PNG){width=500px height=180px fig-align="center"}


---

## Data Distribution

- <span style="font-size:0.7em">For the sake of the theoretical analysis, it is assumed that the sampled DNA strands follow a Poisson distribution.</span>
- <span style="font-size:0.7em">Although the standard Poisson model does not account for synthesis or sequencing biases, it still provides significant insights into strand sampling behavior.</span>

#### Poisson Recap:
- <span style="font-size:0.7em">The Poisson distribution models the number of times an event occurs in a fixed interval, given a known average rate $\lambda$.</span>
- <span style="font-size:0.7em">The probability mass function is given by:</span>

:::{.small_math}  
$$
P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}
$$

:::

## Why Poisson?

::: {.incremental}

- <span style="font-size:0.7em">In the paper, the authors assume that each of the $n \cdot c_r$ sequenced strands is sampled independently and uniformly at random from the $n \cdot c_w$ synthesized strands.

- <span style="font-size:0.7em">This defines a **multinomial distribution** over the $n \cdot c_w$ strands (trials), where each trial has a selection probability of $\frac{1}{n \cdot c_w}$ per draw.

- <span style="font-size:0.7em">When the number of strands is large then the individual selection probability of each strand is small.

- <span style="font-size:0.7em"> Therefore, we can apply the **Poisson approximation to the multinomial**:</span>

  <span style="font-size:0.7em">Each strand is sampled independently according to a $\text{Poisson}(\lambda)$ distribution, where:


:::{.fragment .small_math}
$$
\lambda = \text{#Samples}*\text{Proability}= \frac{nc_r}{nc_w} = \frac{c_r}{c_w}
$$

:::

:::



# Communication and Information Basics

::: {.notes}
- We have shown that the cost of read and cost of write are closely related, and that the coverage is not a good metric to evaluate the tradeoff between them.
- To understand how, we need to understand the theoretical foundations of communication and information theory.
:::
---

## What is a Channel? {data-name="Theoretical Foundations"}

A **channel** is a mathematical model used in information theory to describe how information is transmitted from a sender to a receiver.

- Input $X$: a message of n bits.
- Output $Y$: a possibly altered version of the message

The channel introduce **noise**, leading to errors or loss.



---

## Example: Binary Erasure Channel (BEC) 
- <span style="font-size:0.8em">BEC is a channel where each transmitted bit is erased with probability $\varepsilon$.

- <span style="font-size:0.8em">Input: $n$ bits from $\{0, 1\}$.</span>
- <span style="font-size:0.8em">Output: $n$ bits from $\{0, 1,?\}$, where $?$ denotes an erasure.</span>

<span style="font-size:0.8em">Each bit is either received correctly or erased with probability $\varepsilon$.</span>

![](assets/BEC_example.PNG){width=300px height=300px fig-align="center"}


## Entropy

- <span style="font-size:0.8em">**Entropy** quantifies the uncertainty or randomness in a random variable. </span>
- <span style="font-size:0.8em">Formally, if a discrete random variable $X$ takes values $x_1, \dots, x_n$ with probabilities $p_1, \dots, p_n$ , then:</span>

$$
H(X) = -\sum_{i=1}^n p_i \log_2 p_i
$$

- <span style="font-size:0.8em">The entropy is always between 0 and 1: $$0 \leq H(X) \leq 1$$</span>
- <span style="font-size:0.8em">Higher entropy means more uncertainty.</span>
- <span style="font-size:0.8em">Lower entropy means the variable is more predictable.</span>

---

## Entropy – Continued

- $H(X) = 0$ when $X$ is completely predictable (e.g., a constant value).
- $H(X) = 1$ when $X$ is completley uncertain. This occurs when $X \sim \text{Bern}(\tfrac{1}{2})$.
- Entropy satisfies a version of the law of total probability:

$$
H(X) = \sum_y P(Y = y) \cdot H(X \mid Y = y)
$$

---

## Capacity of a BEC

<span style="font-size:0.7em"> Assume a bit $X\sim\mathrm{Bern}\bigl(\tfrac12\bigr)$ and a Binary Erasure Channel with erasure probability $\varepsilon$. Let $Y$ be the output of the channel. Then:</span>

- <span style="font-size:0.7em"> The entropy of $X$ holds: $H(X)=1$</span>
- <span style="font-size:0.7em"> The conditional entropy of $X$ given $Y$ is:</span>

::: {.small-math}
$$  
\begin{aligned}  
H(X \mid Y)
&= P(Y = ?) \cdot H(X \mid Y = ?) + P(Y \neq ?) \cdot H(X \mid Y \neq ?) \\
&= \varepsilon \cdot 1 + (1 - \varepsilon) \cdot 0 \\
&= \varepsilon
\end{aligned}
$$

:::

- <span style="font-size:0.7em"> This holds because when the output is “?”, then $P(X=1)=P(X=0)=0.5$, which is a state of maximum uncertainty. On the other hand, when the output is 0 or 1, the input is known with maximal certainty by the definition of BEC.</span>

---

<!-- TODO(Mattan): Add hidden slide with mutual information as expectation of the PMI -->

## Capacity of a Binary Erasure Channel (BEC)
**<span style="font-size:1.1em">Mutual information </span>** 

- Mutual information is a measure of how much information the output $Y$ gives on the input $X$.

- It is defined as:
   $$  
   \begin{aligned}  
   I(X;Y)
   &=H(X)\;-\;H(X\mid Y)\\  
   \end{aligned}
   $$

  - In our case: 

    $$  
   \begin{aligned}  
   I(X;Y)
   &=H(X)\;-\;H(X\mid Y)=\\
   &=1\;-\;\varepsilon  
   \end{aligned}
   $$

---

## Capacity of a Binary Erasure Channel (BEC)
**<span style="font-size:1.1em">Capacity </span>**  

- Capacity is the maximum reliable information rate of a channel.
- It tells us what is the maximum number of bits per channel use that can be transmitted reliably.
- The capacity is defined as:
$$
C = \max_{p(x)} I(X; Y)
$$

---

## Capacity of a Binary Erasure Channel (BEC) {.smaller}
**<span style="font-size:1.1em">Capacity - cont.</span>**  

- The maximum mutual information over all input distributions represents the most information the output can infer about the input through the channel.

- This means that up to this rate, information can be transmitted and decoded reliably. No other input distribution can achieve a higher reliable information rate since it's maximal. 
- apperantly, the mutual information of the random variable $X$ and and it's output $Y$ that we previously examined, is maximal among input distributions. 
- Since $I(X;Y)=1-\varepsilon$ then $C=1-\varepsilon$.

---


## What is the Code Rate?

- In coding theory, the **rate** $R$ measures how much **actual information** is **transmitted per channel use**.

- Recall that the write cost $c_w$ is $\frac{|\text{SYN}|}{|\text{DATA}|}$. Therefore:
$$
1/c_w=\frac{|\text{DATA}|}{|\text{SYN}|}= R
$$

- This tells us what is the rate of information inside an encoded bit, which is exactly the definition of rate.

---

## Why Must the Rate Be ≤ Capacity? {.smaller}

- Recall that the **channel capacity** $C$ represents the **maximum number of information bits per symbol** that can be decoded reliably.

- The **code rate** $R$ reflects the **amount of information per channel symbol** that is being transmitted.

- If $R > C$, then the encoder is injecting **more information** per symbol than the channel can reliably preserve — meaning **some information must be lost** or **decoded incorrectly**.

- Therefore, to achieve **reliable communication**, it is necessary that $R\leq C$.

- According to Shannon's theorem, this is a fundamental limit of communication systems, and according to the Noisy-Channel Coding Theorem, it is achievable with the right coding scheme.

---

## Numerical Example
Assume we have a BEC with $\varepsilon=0.6$ and an encoder with $c_w=2$.

- $R=\frac{1}{c_w}=\frac{1}{2}$.
- $C=1-\varepsilon = 1-0.6=0.4$
- It holds that $R=0.5>0.4$
- Therefore, in this model, the data might not be decoded correctly, or might miss some information.

---

## Binary Symmetric Channel (BSC)
- A Binary Symmetric Channel (BSC) is a channel where each bit is flipped with probability $\varepsilon$.
- The difference between BSC and BEC is that in BSC, the output is always a bit (0 or 1), while in BEC, the output can also be an erasure symbol (?). 
- We do not know where the substitution occurred, but we know that it did occur.

![](assets/BSC.PNG){width=400px height=270px fig-align="center"}

## Recap
- The storage model discussed so far is composed on $nc_w$ strands that are synthesized, and $nc_r$ strands that are sampled from a poisson distributed data.

- Then, the data is transmited through a Binary Symmetric Channel (BSC) with substitution rate $\varepsilon$.

- In a Binary Erasure Channel (BEC), the capacity of the channel must be greater or equal to the rate of the code. 

- We now proceed to analyze the lower bounds of the trdeoff between $c_w$ and $c_r$ with different values of $\epsilon$.

---

## The Case $\varepsilon = 0$

- When $\varepsilon = 0$, there are **no sequencing errors**, so each read is error-free.

- Each DNA strand is sampled independently, and the number of times a strand appears follows a **Poisson distribution** with mean $\lambda = \frac{c_r}{c_w}$

- The probability that a given strand is not observed at all (i.e., the Poisson variable equals 0) is:  $\mathbb{P}[\text{strand unseen}] = e^{-\lambda}$

- If a strand is not observed in the read process, we effectively have an **erasure**.

---

## The Case $\varepsilon = 0$ - cont.

- Thus, the channel can be modeled as a **Binary Erasure Channel (BEC)** where the erasure probability is $\varepsilon = e^{-c_r / c_w}$.

- Since the capacity of the BEC must be greater or equal to the rate of the code, we get that: 

$$
R=\frac{1}{c_w}\leq 1-e^{-\frac{c_r}{c_w}}= C
$$

Simplifying the equation, we get the following lower bound for the cost of read:

$$
c_r\geq c_w ln(\frac{c_w}{c_w-1})
$$

- We can see that as $c_w$ increases, $c_r$ decresed, and vice-versa. This fits our intuition. 


## The case $\epsilon \neq 0$ 

<!-- TODO(Atar): Visual example -->

- Since now the reads might have substitution errors, each bit that is sampled is transmitted through a BSC with error probability $\varepsilon$.

- Recall that it is assumed that the index of each strand remains intact, and that each strand is sampled $k$ times, where $k$ is $Poisson(\frac{c_r}{c_w})$ distributed.

- Assume that for each bit in each strand, we are given a tuple ($k_0,k_1$) that indicates how how many times the bit was sampled as 0 and 1, consecutively. Note that $k_0+k_1=k$.

- The probability that the samples of the bit are ($k_0,k_1$) given that the bit has value 0 is:

---

## The case $\epsilon \neq 0$ - cont.

$$
P((k_0,k_1)|0)=\frac{e^{-\lambda}\lambda^{k_0+k_1}}{(k_0+k_1)!} {k_0+k_1\choose k_1} (1-\varepsilon)^{k_0}\varepsilon^{k_1}
$$

* The term $\frac{e^{-\lambda}\lambda^{k_0+k_1}}{(k_0+k_1)!}$ is the probability that a poisoon random variable gets the value $k_0+k_1$. 

* the term ${k_0+k_1\choose k_1} (1-\varepsilon)^{k_0}\varepsilon^{k_1}$ is the binomial probability that there are $k_1$ errors out of $k_0+k_1$ samples. 

- The authors used the above probability in simulations to develop an empirical lower bound on the read cost, $c_r$.

- Since this probability is defined at the bit level rather than the strand level, and since the bound was derived through simulations, it is not guaranteed to be tight.

---

## $c_r$ vs $c_w$: Theoretical bounds 
- This graph demonstrates the lower bounds of the cases $\varepsilon=0$ and $\varepsilon=0.5%$.

- The authors chose $\varepsilon=0.5%$ because this is the substitution probability of illumina sequencing.

![](assets/Graph_1.PNG){width=550px height=450px fig-align="center"}


# Comparison of coding strategies {data-name="Strategies"}

---

## Coding Strategies - Introduction
- In the DNA-based storage world, there is a variety of coding schemes and techniques to recover different types of errors.

- The authors of the paper consider 2 coding strategies:
  * **Inner/Outer Code Separation** - a coding startegy that is composed of an inner code, which handles error inside strands, and outer code, which handles errors in the order of the strands. 
  * **Single Large Block Code** - a coding strategy where large block of inforamtion us encoded, and then it is segmented into strands. 

- Most of works on DNA-based storage prior to this work used the seperation technique, some incorporating elements from the single large block strategy.


## Inner/Outer Code Strategy

::: {.incremental}

- <span style="font-size:0.8em">This strategy uses two layers of coding:

  - <span style="font-size:0.8em">**Outer code**: Applied across information segments to correct **erasures**.
  - <span style="font-size:0.8em">**Inner code**: Applied within each strand to correct **substitution errors**.

- <span style="font-size:0.8em">The decoder groups reads by index and applies **majority voting** per position. Then:
  - <span style="font-size:0.8em">The **inner decoder** corrects strand-level errors.
  - <span style="font-size:0.8em">The **outer decoder** recovers missing segments.

- <span style="font-size:0.8em">This approach is less effective for short blocks, but near-optimal erasure codes exist for large $n$.

:::


![](assets/inner_outer.PNG){width=900px height=180px fig-align="center"}



## Single large block code

::: {.incremental}

- <span style="font-size:0.8em">Instead of using two separate codes, this approach encodes the **entire data block** using one large **LDPC (Low-Density Parity-Check) code**, and then seperates the encoded block into segments. </span>

- <span style="font-size:0.8em">Each segment contains a small portion of a single, long LDPC codeword. </span>

- <span style="font-size:0.8em">During decoding:</span>
  - <span style="font-size:0.8em">All sequenced strands are aligned using their **indices**.</span>
  - <span style="font-size:0.8em">The collected data is treated as a **noisy, partial version** of the LDPC codeword.</span>
  - <span style="font-size:0.8em">A **belief propagation** algorithm is used to recover the full data block.</span>

- <span style="font-size:0.8em">This method enables a better read/write cost tradeoff.</span>

:::
 

![](assets/large_block.PNG){width=600px height=190px fig-align="center"}



## $c_r$ vs $c_w$: Simulation Bounds (Separated vs. Large Block)

- <span style="font-size:0.8em">The graph shows the performance of both coding strategies—**separated blocks** and a **single large block**—compared to the theoretical bound. </span>

- <span style="font-size:0.8em">The simulation uses the parameters: $n = 1000$, $L = 256$. </span>


- <span style="font-size:0.8em">The authors used a **BCH code** as the inner code and a **Raptor code** as the outer code—both known to perform well under the chosen settings. </span>

![](assets/Graph_2.PNG){width=700px height=400px fig-align="center"}



# Low Density Pairity Check Codes (LDPC)

## LDPC Codes {data-name="LDPC"}

- LDPC (Low-Density Parity-Check) codes are a powerful class of linear error-correcting codes.
- First introduced by Gallager in the 1960s, but gained widespread adoption in the 1990s with improved decoding algorithms.
- They are now used in modern systems such as Wi-Fi, 5G, satellite communication, flash storage, and digital broadcasting.
- LDPC codes achieve performance close to the Shannon limit, making them highly efficient for reliable communication.
- They support fast and scalable decoding using iterative message-passing algorithms.


## LDPC Motivation
Assume we have the following data to be transmitted:

::: {.big-math}
$$
1\hspace{1mm}0\hspace{1mm}0\hspace{1mm}1\hspace{1mm}1\hspace{1mm}0
$$
:::

and after we transimetted the message, it was recieved as:

::: {.big-math}
$$
1\hspace{1mm}?\hspace{1mm}0\hspace{1mm}1\hspace{1mm}1\hspace{1mm}0
$$
:::

- What can help us recover the erasured bit?

---

## Pairity Check
We can add a pairity check bit to our data:

<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 0 0 1 1 0 <span class="fragment" style="color:red;">1</span>
</div>
<br><br>
<span class="fragment">
 Now, when we will recieve the following data:
 <div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 ? 0 1 1 0 <span style="color:red;">1</span>
</div>
</span>
<br>

<span class="fragment">
We can calculate the pairity of all bits besides the erasured bit, compare it to the pairity check bit, and thus recover the erasured bit.
</span>

---

## What happens if more than 1 erasure?

Assume with have the same data as before with the pairity check bit:

<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 0 0 1 1 0 <span style="color:red;">1</span>
</div> <br>

And now, after transmitting this data, the data recieved is:<br><br>
<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 ? 0 1 ? 0 <span style="color:red;">1</span>
</div> <br>
Can our pairity check bit help us recover both erasured bits?

---

## Naive Idea

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.

<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 0 0 1 1 0
</div><br>

---

## Naive Idea

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.

![](assets/ldpc_ex1.PNG){width=545px height=290px fig-align="center"}

<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
<span class="fragment"> 1 0 0 1 1 0 <span style="color:red;">1</span></span>
</div><br>

---

## Naive Idea

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.

![](assets/ldpc_ex1.PNG){width=545px height=290px fig-align="center"}

<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
![](assets/ldpc_ex2.PNG){width=470px height=290px fig-align="center"}
</div>
---

## Naive Idea

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.

![](assets/ldpc_ex1.PNG){width=545px height=290px fig-align="center"}

<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 0 0 1 1 0 <span style="color:red;">1 1</span>
</div><br>

---

## Naive Idea

So now, when the same erasures as before will occur:
<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 ? 0 1 ? 0 <span style="color:red;">1 1</span>
</div><br>
We will be able to recover the erasured bits.
<br><br>
<div class="fragment">
Are we done? 
<br><br>
</div>
<div class="fragment">
What if the erasured data will be<br><br>
<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 ? ? 1 1 0 <span style="color:red;">1 1</span>
</div></div>

---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa1.PNG){width=900px height=400px}





---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa2.PNG){width=900px height=400px}


## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa3.PNG){width=900px height=400px}

---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa4.PNG){width=900px height=400px}

---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa5.PNG){width=900px height=400px}

---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa6.PNG){width=900px height=400px}

---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa7.PNG){width=900px height=400px}

---

## Possible Solution  
### Overlapping Parity Checks

- <span style="font-size:0.8em">In the overlapping parity check example, we showed how to correct any 2 erasures in a 4-bit message. </span>

- <span style="font-size:0.8em">What would happen if we apply this strategy repeatedly over consecutive 4-bit blocks in a longer message?</span>

![](assets/4-bit_ex.PNG){width=600px height=150px fig-align="center"}

- <span style="font-size:0.8em">And what if we increase the overlap size together with the amount of pairity checks per overlap — how does that impact performance and error correction?</span>

![](assets/16-bit_ex.PNG){width=800px height=140px fig-align="center"}

---

## Possible Solution
### Parity Check Bits Protection

- A natural question arises: how are the **parity check bits** protected, given that they are also subject to substitution errors?

- The solution lies in the **structure of LDPC codes**:  
  Parity bits are **not isolated**—they are also included in other parity check equations.

- By involving **both data and parity bits** in the parity constraints, LDPC codes ensure that **parity bits are themselves protected** and can be recovered during decoding.

- This design enables the **belief propagation algorithm** to iteratively correct errors in both data and parity bits, and effectively recover all the data.


## Density and the Role of Overlap

- Therefore, our goal is to design a parity-based error correction code that:
  - Has **low write cost**,  
  - Enables **fast and efficient decoding**,  
  - **recovers erasures effectively**.

- In the 1960s, Robert Gallager introduced a groundbreaking approach for this challenge — now known as LDPC codes (Low-Density Parity-Check codes).

<!-- TODO(Atar): Visualize? -->

---

## Density and the Role of Overlap
- Gallager showed how to **balance two competing goals**:
  - **High recovery power** (which favors larger overlapping neighborhoods),
  - With **low decoding complexity** (which favors smaller overlaps).

- He introduced the concept of **density**, which is a measure of how many bits participate in each parity check:
  - **Higher density** means more overlap (stronger recovery),
  - **Lower density** means less overlap (faster decoding).

---

:::{.smaller}
## Low Density

::: {.incremental}

- Gallager showed that even **sparse** parity-check matrices can achieve strong error correction.

- Low-density codes allow efficient **decoding algorithms** like belief propagation.

- Sparsity reduces complexity while still maintaining performance close to capacity.

- Gallager also showed that **randomly generated** sparse matrices usually perform well.

- Randomness avoids problematic patterns in the code structure.

- In practice, carefully designed random LDPC matrices approach the Shannon limit.

:::

:::



## LDPC Theory

- A LDPC code can be presented as a parity-check matrix $H$.

- The matrix is composed of $n$ columns and $k$ rows, where each column represents a bit (pairity bit or regular bit), and each row represents a parity-check equation over some bits.

- Each row has only a few non-zero entries, which mean that the pairity checks have low Density.

- A vector $\bar{v}\in \{0,1\}^{nX1}$ is a codeward if and only if $Hv=\bar{0}$.

### Example

Let $n = 8$, and $k = 3$. Then one possible $H$ matrix:

$$
H =
\begin{bmatrix}
1 & 1 & 0 & 1 & 0 & 0 & 0 & 0\\
0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 & 1 & 1 & 0
\end{bmatrix}
$$

::: {.fragment}
Each row says: the sum (mod 2) of the marked bits must equal zero.  
These constraints define the valid codewords.
:::

## LDPC - Encoding "Algorithm"

## LtDPC - Decoding Algorithm 
### Gallager’s bit-flipping algorihm
  - Gallager’s bit-flipping algorithm (BSC)

## LDPC Decoding: Belief propagation algorithm

## Encoding Schema
::: {.incremental}
- Reads
- Addressing index+BCH
- MSA 
- Sync Markers
- LDPC
:::

## Experimental results {data-name="Experimental results"}
::: {.incremental}
- Figure 8
- Error profiles (4.1.1)
- Deviation from poisson distribution in number of reads (4.1.1)
- Probability of decoding failure (4.1.5)
- Stress testing (4.1.6)
- Performance and Scalability (4.1.7)
- Simulations based on [github](https://github.com/shubhamchandak94/LDPC_DNA_storage) code
- Simulation results (Graph 4)
- Idea (probably not): Implement a small subset of the simulation and explain 1
:::

## Conclusions {data-name="Conclusion"}

## Crafting the Presentation: Tools
- [`Quarto`](https://quarto.org/): markdown-based authoring system that supports multiple output formats.
- [`revealjs`](https://revealjs.com/): a framework for creating interactive presentations using HTML and JavaScript.
  - [`simplemenu`](https://github.com/Martinomagnifico/quarto-simplemenu): a plugin to create a menu bar that allows us to navigate through the presentation.

## Ideas

- Gallager youtube talk at the end of the slides
- Example of LDPC using [pyldpc](https://hichamjanati.github.io/pyldpc/auto_examples/plot_coding_decoding_simulation.html#sphx-glr-auto-examples-plot-coding-decoding-simulation-py)