---
title: "Seminar on DNA Data Storage"
subtitle: "Improved read/write cost tradeoff in DNA-based data storage using LDPC codes<br><br>Shubham Chandak, Kedar Tatwawadi, Billy Lau, Jay Mardia, Matthew Kubit, Joachim Neu, Peter Griffin, Mary Wootters, Tsachy Weissman, Hanlee J<br>(2019)<br><br>Presented by: Atar Ron and Mattan Hoory"
format:
  revealjs: 
    title-slide-attributes:
      data-state: "hide-menubar"
    slide-number: true
    preview-links: auto
    css: style.css
    logo: assets/CS_LOGO.jpg
    footer: 'DNA Data Storage - 02360801 - Spring 2025'
    toc: true
    toc-depth: 1
    simplemenu:
        flat: true
        barhtml:
            header: "<div class='menubar'><ul class='menu'></ul><div>"
        scale: 0.42

revealjs-plugins:
  - simplemenu
---
  
::: {.center}
# 1 Introduction & Context {data-name="Intro"}
:::

## Background

<span style="font-size:0.7em"> In DNA-based data storage, there are two critical challenges: </span>

::: {.incremental}

- <span style="font-size:0.7em">**Write cost** — how much synthetic DNA we need to store one bit of information. </span>
- <span style="font-size:0.7em">**Read cost** — how many reads are needed to reliably recover that bit.</span>


<span style="font-size:0.7em" class= "fragment"> These two costs are closely related, and thus, one of the most important problems in DNA data storage is finding an effective tradeoff between them. </span>

<div style="text-align: center;">
  <img src="assets/write_read_proc.PNG" width="400px" height="200px" class="fragment">
</div>

:::

---

## Discussion Questions

::: {.incremental}

- Why , in your opinion, are the cost of write and cost of read closely related? how does the write cost affect the read cost?
&nbsp; &nbsp;  
- What are the main factors that affect the cost of write?  
&nbsp; &nbsp;  
- What are the main factors that affect the cost of read?

:::


---

## Main Goals

::: {.incremental}

Since the cost of write and cost of read are strongly correlated, this paper aims to:


- Establish a theoretical lower bound on the tradeoff between write cost and read cost.
- Design and evaluate a practical coding scheme (based on LDPC codes) that achieves a better tradeoff than previous methods.
- Validate the performance of the scheme through both real experiments (DNA synthesis and sequencing) and simulations.

:::

## Cost of Read and Write 

* **Cost Of Write**
  Average number of encoded bits synthesized per information bit:

  $$
  c_w = \frac{\text{#Synthesized bits}}{\text{#Data}}
  $$

* **Cost of Read**
  Average number of bits read per information bit:

  $$
  c_r = \frac{\text{#Read bits}}{\text{#Data bits}}
  $$

## Coverage

* <span style="font-size:0.7em">**Coverage**- Average number of bits read per synthesized bit:</span>


<div style="font-size: 0.8em;">


  $$
  \text{Coverage} = \frac{\text{#Read Bits}}{\text{#Synthesized bits}}
  = \frac{\frac{\text{#Read bits}}{\text{#Data bits}}}{\frac{\text{#Synthesized bits}}{\text{#Data bits}}}
  = \frac{c_r}{c_w}
  $$

</div>


::: {.incremental}
- <span style="font-size:0.7em">Coverage has been widely used in prior work to estimate the efficiency of read/write tradeoffs in DNA-based storage systems. 
<br> </span>

- <span style="font-size:0.7em">It measures how many sequencing reads are made per synthesized bit — but is that the best way to evaluate system performance?
</span>

:::
---

## Is Coverage a Good Metric?

<span style="font-size:0.7em">We consider the following example that demonstrates why coverage can be misleading.</span><br>

### Example
<span style="font-size:0.7em">Suppose we compare two storage systems with the following properties:</span>

| System        | $c_w$ | $c_r$ | $\text{coverage} = c_r/c_w$ |
|----------------|------------|----------------|------------------------|
| A  | 4                   | 12                 | 3                   |
| B  | 2                   | 10                 | 5                   |

* <span style="font-size:0.7em">**Note:** In this example, we assume that both the read cost and the coverage values were measured after decoding all the sampled strands.</span>



## Example - cont. 

| System        | $c_w$ | $c_r$ | $\text{coverage} = c_r/c_w$ |
|----------------|------------|----------------|------------------------|
| A  | 4                   | 12                 | 3                   |
| B  | 2                   | 10                 | 5                   |

* <span style="font-size:0.8em">At first glance, System A has better (lower) coverage. Therefore, when evaluating the systems based on coverage only, we will prefer System A. </span>  


* <span style="font-size:0.8em"> But, it is easy to see that System B reads fewer total bits per information bit (10 vs. 12), and also synthisizes significantly less DNA. So despite having higher coverage, System B is clearly more efficient overall. </span>


---

## Example - cont. 
<br>
**Conclusion:** <span style="font-size:0.9em">This example illustrates that relying solely on coverage can be misleading, particularly when comparing systems with varying write costs. A more meaningful comparison is to evaluate the actual read and write costs per information bit.</span>  
<br>

**Important Note:** <span style="font-size:0.9em">We can also define coverage at the strand level: the average number of times a strand is observed in the sampled reads. We'll refer back to this concept later on.</span>


# The Model


---

## Model Notations

- $n$ – Number of strands.  


- $L$ – Strand length. 


- $c_{w}$ – Cost of write. 


- $c_{r}$ – Cost of read.

- $\epsilon$ - Substitution error rate.


## Model Definition {.nostretch}

::: {.fragment data-fragment-index="1"}
- <span style="font-size:0.7em; color:#00B0F0; line-height: 1.0; display: inline-block; margin-bottom: 4px;">
    The storage system encodes $n$ information strands, each of length $L$, resulting in a total of $nL$ data bits.
  </span>
:::

::: {.fragment data-fragment-index="3"}
- <span style="font-size:0.7em; color:#059B2C; line-height: 1.0; display: inline-block; margin-bottom: 4px;">
    Each data bit leads to the synthesis of $c_w$ bits. Thus, encoding $nL$ data bits requires synthesizing $nc_wL$ bits, which are grouped into $nc_w$ strands.
  </span>
:::

::: {.fragment data-fragment-index="5"}
- <span style="font-size:0.7em; color:#059B2C; line-height: 1.0; display: inline-block; margin-bottom: 4px;">
    Reading each data bit involves sampling $c_r$ synthesized bits. Therefore, $nL$ data bits require sampling of $nc_rL$ synthesized bits, which can be organized into $nc_r$ strands.
  </span>
:::


<!-- Images side-by-side and controlled with fragments -->
<div style="margin-bottom: 200px;">
  <img src="assets/model_process1.PNG" width="280" height="200" class="fragment" data-fragment-index="2">
  <img src="assets/model_process2.PNG" width="350" height="200" class="fragment" data-fragment-index="4">
  <img src="assets/model_process3.PNG" width="370" height="200" class="fragment" data-fragment-index="6">
</div>


---


## Model Definition - cont.
- <span style="font-size:0.6em"> It is assumed, for simplicity, that the decoder has access to the index of each strand, and that deletion and insertion errors are ignored. We later explain how the authors overcame this in practice. </span>

- <span style="font-size:0.6em" > The reads are subject to:</span>

  - <span style="font-size:0.6em"><strong>Substitution errors</strong> — each sampled bit is flipped with probability $\epsilon$ (modeled as a BSC, discussed later).</span>
  - <span style="font-size:0.6em"><strong>Sampling variability</strong> — the number of times each strand is sampled is random and follows a Poisson distribution.</span>


![](assets/model_process4.PNG){width=700px height=120px fig-align="center"}


---

## Data Distribution

<div style="font-size:0.9em; line-height: 1.2;">

- For the sake of the theoretical analysis, it is assumed that the sampled DNA strands follow a Poisson distribution.
- Does not account for synthesis or sequencing biases, but still provides significant insights into strand sampling behavior.

#### Poisson Recap:
- The Poisson distribution models the number of times an event occurs in a fixed interval, given a known average rate $\lambda$.
- The probability mass function is given by:

</div>

<div style="font-size:0.8em">
$$
P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}
$$

</div>


## Why Poisson?

::: {.incremental}

<div style="font-size:0.7em; line-height: 1.1;">

- The authors assume that each of the $n \cdot c_r$ sequenced strands is selected independently and uniformly at random from the $n \cdot c_w$ synthesized strands — with replacement.</span>

- This results in a **multinomial distribution**: each of the $n \cdot c_w$ strands has a fixed probability $\frac{1}{n \cdot c_w}$ of being chosen in any given draw.</span>

- When the number of synthesized strands is large, the probability of selecting any particular strand becomes very small, but the number of trials remains large.</span>

- In this case, the multinomial distribution can be approximated with **independent Poisson distributions** where:</span>
  Each strand is independently selected according to a $\text{Poisson}(\lambda)$ distribution where:

</div>


::: {.fragment}
<div style="font-size: 0.7em">
$$
\lambda = \text{#Samples} \cdot \text{Probability} = \frac{nc_r}{nc_w} = \frac{c_r}{c_w}
$$
</div>
:::


:::


::: {.center}
# 2 Communication and Information 101 {data-name="Theory"}
:::

---


## Communication and Information - Motivation

<div style="font-size:0.9em; line-height: 1.7;">

- We’ve seen that the **write cost** $c_w$ and **read cost** $c_r$ are tightly connected. With an optimal code, increasing $c_w$ typically reduces $c_r$.

- However, a higher write cost means more redundancy, which **lowers the information rate** per bit.

- In this section, we introduce basic **channel theory** to better understand and analyze the **tradeoff between $c_w$ and $c_r$**.

</div>

::: {.notes}
- We have shown that the cost of read and cost of write are closely related, and that the coverage is not a good metric to evaluate the tradeoff between them.
- To understand how, we need to understand the theoretical foundations of communication and information theory.
:::
---

## What is a Channel?


<div style="font-size:0.9em; line-height: 1.2;">

A **channel** is a mathematical model used in information theory to describe how information is transmitted from a sender to a receiver.

- Input $X$: a message of n bits.
- Output $Y$: a possibly altered version of the message

The channel introduce **noise**, leading to errors or loss.

</div>

![](assets/channel.PNG){width=600px height=250px fig-align="center"}

---

## Channel Examples 
- <span style="font-size:0.7em;">Binary Erasure Channel (BEC):</span>  
  <div style="margin-left: 400px; margin-top: -10px; margin-bottom: 5px; text-align: center;">
    <img src="assets/BEC_example.PNG" width="300px" height="200px" style="display: inline-block;">
  </div>

- <span style="font-size:0.7em;">Binary Symmetric Channel (BSC):</span>  
  <div style="margin-left: 400px; margin-top: -10px; margin-bottom: 5px; text-align: center;">
    <img src="assets/BSC.PNG" width="300px" height="200px" style="display: inline-block;">
  </div>

<!--
## Entropy

- <span style="font-size:0.8em">**Entropy** quantifies the uncertainty or randomness in a random variable. </span>
- <span style="font-size:0.8em">Formally, if a discrete random variable $X$ takes values $x_1, \dots, x_n$ with probabilities $p_1, \dots, p_n$ , then:</span>

$$
H(X) = -\sum_{i=1}^n p_i \log_2 p_i
$$

- <span style="font-size:0.8em">The entropy is always between 0 and 1: $$0 \leq H(X) \leq 1$$</span>
- <span style="font-size:0.8em">Higher entropy means more uncertainty.</span>
- <span style="font-size:0.8em">Lower entropy means the variable is more predictable.</span>

---

## Entropy – Cont.

- $H(X) = 0$ when $X$ is completely predictable (e.g., a constant value).
- $H(X) = 1$ when $X$ is completley uncertain. This occurs when $X \sim \text{Bern}(\tfrac{1}{2})$.
- Entropy satisfies a version of the law of total probability:

$$
H(X) = \sum_y P(Y = y) \cdot H(X \mid Y = y)
$$

![](assets/entropy.PNG){width=600px height=300px fig-align="center"}

---

## Capacity of a Binary Erasure Channel (BEC)

<span style="font-size:0.7em"> Assume a bit $X\sim\mathrm{Bern}\bigl(\tfrac12\bigr)$ and a Binary Erasure Channel with erasure probability $\varepsilon$. Let $Y$ be the output of the channel. Then:</span>

- <span style="font-size:0.7em"> The entropy of $X$ holds: $H(X)=1$</span>
- <span style="font-size:0.7em"> The conditional entropy of $X$ given $Y$ is:</span>

::: {.small-math}
$$  
\begin{aligned}  
H(X \mid Y)
&= P(Y = ?) \cdot H(X \mid Y = ?) + P(Y \neq ?) \cdot H(X \mid Y \neq ?) \\
&= \varepsilon \cdot 1 + (1 - \varepsilon) \cdot 0 \\
&= \varepsilon
\end{aligned}
$$

:::

- <span style="font-size:0.7em">This holds because when the output is "?"", the input is completely uncertain, i.e., $P(X=1) = P(X=0) = 0.5$. In contrast, when the output is 0 or 1, the input is known with certainty, as defined by the Binary Erasure Channel (BEC).</span>


---

## Capacity of a Binary Erasure Channel (BEC)
**<span style="font-size:1.1em">Mutual information </span>** 

- Mutual information is a measure of how much information the output $Y$ gives on the input $X$.

- It is defined as:
   $$  
   \begin{aligned}  
   I(X;Y)
   &=H(X)\;-\;H(X\mid Y)\\  
   \end{aligned}
   $$

  - In our case: 

    $$  
   \begin{aligned}  
   I(X;Y)
   &=H(X)\;-\;H(X\mid Y)=\\
   &=1\;-\;\varepsilon  
   \end{aligned}
   $$

---

-->

## Capacity of a Binary Erasure Channel (BEC)
  

<div style="font-size:0.9em; line-height: 1.2;">

- Capacity is the highest rate at which information can be reliably transmitted over a communication channel.<br><br>

- It quantifies the maximum rate that can be recovered reliably from the channel's output, per transmitted bit.<br><br>

- For a Binary Erasure Channel (BEC) with erasure probability $\varepsilon$, the capacity is:  
$$
C = 1 - \varepsilon
$$

</div>

---

## Code Rate

<div style="font-size:0.8em; line-height: 1.1;">

- In coding theory, the **code rate** $R$ quantifies the rate of **information** transmitted per **encoded bit**.

- Recall that the write cost $c_w$ is defined as:  
  $$
  c_w = \frac{|\text{SYN}|}{|\text{DATA}|}
  $$
  which implies:
  $$
  \frac{1}{c_w} = \frac{|\text{DATA}|}{|\text{SYN}|} = R
  $$

- In other words, the **inverse write cost** captures how much information is packed into each encoded bit, which is the code rate.

</div>

---

## Shannon's Theorem

<div style="font-size:0.9em; line-height: 1.1;">

- <u>Shannon’s Theorem</u>: Reliable communication over a noisy channel is possible **if and only if** the code rate $R$ satisfies $R \leq C$, where $C$ is the channel’s capacity.<br>
<br>
- Intuitively, if $R > C$, then the rate of inforamtion in each bit is higher than the rate that can be reliably recovered. Therefore, some of it must be lost or corrupted.<br>
<br>
- However, if $R \leq C$, Shannon proved that with a suitable coding scheme and long enough messages, the error probability can be made arbitrarily small.

</div>

---

## Numerical Example

Assume we have a BEC with $\varepsilon=0.6$ and an encoder with $c_w=2$.

- $R=\frac{1}{c_w}=\frac{1}{2}$.
- $C=1-\varepsilon = 1-0.6=0.4$
- It holds that $R=0.5>0.4$
- Therefore, in this model, the data might not be decoded correctly, or might miss some information.

---

## Recap
- The storage model consists of $nc_w$ synthesized strands and $nc_r$ sampled strands. Each strand is sampled a number of times drawn from a $\text{Poisson}(\lambda)$ distribution. The index of each strand is assumed to remain intact.


- The data is transmited through a Binary Symmetric Channel (BSC) with substitution rate $\varepsilon$.

- In a Binary Erasure Channel (BEC), the capacity of the channel must be greater or equal to the rate of the code. 

    
::: {.center}
# 3 Theoretical bounds {data-name="Bounds"}
:::

## The Case $\varepsilon = 0$

<div style="font-size:0.9em; line-height: 1.2;">

- When $\varepsilon = 0$, there are **no sequencing errors**, so each read is error-free.

- Each DNA strand is sampled independently, and the number of times a strand appears follows a **Poisson distribution** with mean $\lambda = \frac{c_r}{c_w}$.

- The probability that a given strand is sampled 0 times is:  $\mathbb{P}[X=0] = e^{-\lambda}$. If a strand is not observed in the read process, we effectively have an **erasure**.

- Thus, the model can be modeled as a **Binary Erasure Channel (BEC)** where the erasure probability is $\varepsilon =e^{-c_r / c_w}$.

</div>

---

## The Case $\varepsilon = 0$ - cont.

<div style="font-size:0.8em; line-height: 1.2;">

- Since the capacity of the BEC must be greater or equal to the rate of the code, we get that: 

$$
R=\frac{1}{c_w}\leq 1-e^{-\frac{c_r}{c_w}}= C
$$

Simplifying the equation, we get the following lower bound for the cost of read:

$$
c_r\geq c_w ln(\frac{c_w}{c_w-1})
$$

- We can see that as $c_w$ increases, $c_r$ decresed, and vice-versa. This fits our intuition. 

</div>


## The Case $\varepsilon \neq 0$ 

<div style="font-size:0.9em; line-height: 1.2;">

- Now, each sampled bit is transmitted through a BSC with error probability $\varepsilon>0$.

- Recall that it is assumed that the index of each strand remains intact, and that each strand is sampled $k$ times, where $k$ is $Poisson(\frac{c_r}{c_w})$ distributed.

- Assume that for each bit in each strand, we are given a tuple $(k_0,k_1)$ that indicates how how many times the bit was sampled as 0 and 1, consecutively. Note that $k_0+k_1=k$.

- The probability that the samples of the bit are ($k_0,k_1$) given that the bit has value 0 is:

</div>

---

## The case $\varepsilon \neq 0$ - cont.

<div style="font-size:0.8em; line-height: 1.2;">

$$
P((k_0,k_1)|0)=\frac{e^{-\lambda}\lambda^{k_0+k_1}}{(k_0+k_1)!} {k_0+k_1\choose k_1} (1-\varepsilon)^{k_0}\varepsilon^{k_1}
$$

* The term $\frac{e^{-\lambda}\lambda^{k_0+k_1}}{(k_0+k_1)!}$ is the probability that a poisson random variable gets the value $k_0+k_1$. 

* the term ${k_0+k_1\choose k_1} (1-\varepsilon)^{k_0}\varepsilon^{k_1}$ is the binomial probability that there are $k_1$ errors out of $k_0+k_1$ samples. 

* The probability of observing the sample counts $(k_0, k_1)$ given that the bit is 1 is symmetric with respect to swapping $k_0$ and $k_1$.

* The following example illustrates how the calculations are performed.

</div>

---

## The case $\varepsilon \neq 0$ - Example

- <span style="font-size:0.8em"> In this example, we assume that $\epsilon=0.1$ and $\lambda=3$. </span>


::: {.center}

<div style="font-size: 0.8em;">

<table style="font-family: monospace; font-size: 1.1em; border-collapse: separate; border-spacing: 0 6px;">
  <tr style="border-bottom: 2px solid #999;">
    <th style="padding: 0px 12px;">Source</th>
    <th style="padding: 0px 12px;">Bit 1</th>
    <th style="padding: 0px 12px;">Bit 2</th>
    <th style="padding: 0px 12px;">Bit 3</th>
    <th style="padding: 0px 12px;">Bit 4</th>
  </tr>
  <tr>
    <td style="padding: 0px 12px;">Original</td>
    <td style="color: black; text-align: center;">1</td>
    <td style="color: black; text-align: center;">0</td>
    <td style="color: black; text-align: center;">1</td>
    <td style="color: black; text-align: center;">0</td>
  </tr>
  <tr>
    <td style="padding: 0px 12px;">Sample 1</td>
    <td style="color: blue; text-align: center;">1</td>
    <td style="color: red; text-align: center;">0</td>
    <td style="color: blue; text-align: center;">1</td>
    <td style="color: red; text-align: center;">0</td>
  </tr>
  <tr>
    <td style="padding: 0px 12px;">Sample 2</td>
    <td style="color: blue; text-align: center;">1</td>
    <td style="color: blue; text-align: center;">1</td>
    <td style="color: blue; text-align: center;">1</td>
    <td style="color: red; text-align: center;">0</td>
  </tr>
  <tr>
    <td style="padding: 0px 12px;">Sample 3</td>
    <td style="color: red; text-align: center;">0</td>
    <td style="color: red; text-align: center;">0</td>
    <td style="color: blue; text-align: center;">1</td>
    <td style="color: red; text-align: center;">0</td>
  </tr>
  <tr>
    <td style="padding: 0px 12px;">$(k_0,k_1)$</td>
    <td>(1,2)</td>
    <td>(2,1)</td>
    <td>(0,3)</td>
    <td>(3,0)</td>
  </tr>
  <tr>
    <td style="padding: 0px 12px;">Probability</td>
    <td>0.054</td>
    <td>0.054</td>
    <td>0.163</td>
    <td>0.163</td>
  </tr>
</table>

</div>
:::



---

## The case $\varepsilon \neq 0$ - cont.

- The authors adopt a per-bit perspective to analyze the lower bound on performance.

- They use the bit-level probability in simulations to empirically estimate a lower bound on the read cost, $c_r$.

- Since the analysis is performed at the bit level rather than the strand level, and the bound is based on simulation results, it is important to note that the bound is empirical and may not be tight.


## $c_r$ vs $c_w$: Theoretical bounds 
<br>

![](assets/Graph_1.PNG){width=820px height=450px fig-align="center"}


::: {.center}
# 4 Comparison of coding strategies {data-name="Encoding Strategies"}
:::



## Coding Strategies - Introduction

When designing an encoding and decoding scheme, we must account for **two primary sources of error**:

- **Outer Code Errors** — Since the data is divided into multiple strands, some strands may never be sampled or sequenced. The outer code must handle such **erasure errors** and enable recovery of the missing data.

- **Inner Code Errors** — During synthesis and sequencing, strands may undergo **substitution**, **insertion**, or **deletion** errors. The inner code must correct these errors to recover the original encoded information.


---

<!--
## Coding Strategies

- In the DNA-based storage world, there is a variety of coding schemes and techniques to recover inner and outer code errors.

- The authors of the paper consider 2 coding strategies:
  * **Inner/Outer Code Separation** - a coding startegy that is composed of an inner code, which handles error inside strands, and outer code, which handles errors in the order of the strands. 
  * **Single Large Block Code** - a coding strategy where large block of inforamtion us encoded, and then it is segmented into strands. 

- Most of works on DNA-based storage prior to this work used the seperation technique, some incorporating elements from the single large block strategy.


Waiting for Mattan's illustrations of the coding strategies.




## Inner/Outer Code Strategy




::: {.incremental}

- <span style="font-size:0.8em">This strategy uses two layers of coding:

  - <span style="font-size:0.8em">**Outer code**: Applied across information segments to correct **erasures**.
  - <span style="font-size:0.8em">**Inner code**: Applied within each strand to correct **substitution errors**.

- <span style="font-size:0.8em">The decoder groups reads by index and applies **majority voting** per position. Then:
  - <span style="font-size:0.8em">The **inner decoder** corrects strand-level errors.
  - <span style="font-size:0.8em">The **outer decoder** recovers missing segments.

- <span style="font-size:0.8em">This approach is less effective for short blocks, but near-optimal erasure codes exist for large $n$.

:::


![](assets/inner_outer.PNG){width=900px height=180px fig-align="center"}



## Single large block code

::: {.incremental}

- <span style="font-size:0.8em">Instead of using two separate codes, this approach encodes the **entire data block** using one large **LDPC (Low-Density Parity-Check) code**, and then seperates the encoded block into segments. </span>

- <span style="font-size:0.8em">Each segment contains a small portion of a single, long LDPC codeword. </span>

- <span style="font-size:0.8em">During decoding:</span>
  - <span style="font-size:0.8em">All sequenced strands are aligned using their **indices**.</span>
  - <span style="font-size:0.8em">The collected data is treated as a **noisy, partial version** of the LDPC codeword.</span>
  - <span style="font-size:0.8em">A **belief propagation** algorithm is used to recover the full data block.</span>

- <span style="font-size:0.8em">This method enables a better read/write cost tradeoff.</span>

:::
 


![](assets/large_block.PNG){width=600px height=190px fig-align="center"}

-->


---

## Coding Strategies - Summary

- The following table summarizes the role of each type of code in handling **erasure** and **substitution** errors:

<br>

::: center
| Code Type        | Erasure Errors | Substitution Errors |
|------------------|----------------|----------------------|
| Inner Code       | ✗              | ✓                    |
| Outer Code       | ✓              | ✗                    |
| Large Block Code | ✓              | ✓                    |
:::

---

## $c_r$ vs $c_w$: Simulation Bounds (Separated vs. Large Block)

<div style="font-size:0.6em; line-height: 1.0;">

![](assets/Graph_2.PNG){width=700px height=400px fig-align="center"}

- The parameters used in the simulation are: $n = 1000$, $L = 256$.
</div>


::: {.center}
# 5 Low Density Parity Check (LDPC) {data-name="LDPC"}
:::
## LDPC Codes

<div style="font-size:0.9em; line-height: 1.4;">

- LDPC (Low-Density Parity-Check) codes are a powerful class of linear error-correcting codes.
- First introduced by Gallager in the 1960s, but gained widespread adoption in the 1990s with improved decoding algorithms.
- They are now used in modern systems such as Wi-Fi, 5G, satellite communication, flash storage, and digital broadcasting.
- LDPC codes achieve performance close to the Shannon limit, making them highly efficient for reliable communication.
- They support fast and scalable decoding using iterative message-passing algorithms.

</div>

## LDPC Motivation

<div style="font-size:1.0em; line-height: 1.5;">


Assume we have the following data to be transmitted:

::: {.big-math}
$$
1\hspace{1mm}0\hspace{1mm}0\hspace{1mm}1\hspace{1mm}1\hspace{1mm}0
$$
:::

and after we transimetted the message, it was recieved as:

::: {.big-math}
$$
1\hspace{1mm}?\hspace{1mm}0\hspace{1mm}1\hspace{1mm}1\hspace{1mm}0
$$
:::

What can help us recover the erasured bit?

</div>

---

## Pairity Check

<div style="font-size:0.9em; line-height: 1.2;">

We can enhance our data by adding a parity check bit:

<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 0 0 1 1 0 <span class="fragment" style="color:red;">1</span>
</div>

<br><br>

<span class="fragment">
Suppose we later receive the following (with one bit erased):
<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 ? 0 1 1 0 <span style="color:red;">1</span>
</div>
</span>

<br>

<span class="fragment">
To recover the missing bit, we compute the parity of the known bits and compare it to the parity check bit. This allows us to deduce the value of the erased bit.
</span>

</div>


---

## What happens if more than 1 erasure?

Assume with have the same data as before with the pairity check bit:

<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 0 0 1 1 0 <span style="color:red;">1</span>
</div> <br>

And now, after transmitting this data, the data recieved is:<br><br>
<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 ? 0 1 ? 0 <span style="color:red;">1</span>
</div> <br>
Can our pairity check bit help us recover both erasured bits?

---

## Naive Idea

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.

![](assets/naive1.PNG){width=600px height=290px fig-align="center"}

---

## Naive Idea

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.


![](assets/naive2.PNG){width=600px height=290px fig-align="center"}

---

## Naive Idea

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.

![](assets/naive3.PNG){width=600px height=290px fig-align="center"}

---

## Naive Idea

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.

![](assets/naive4.PNG){width=600px height=290px fig-align="center"}

---

## Naive Idea

We can parttion the message into 2 distinct parts, and add a pairity check bit for every part.

![](assets/naive5.PNG){width=600px height=290px fig-align="center"}

<div style="font-size:0.8em; line-height: 1.2;">
<span class="fragment"> So now, when the same erasures as before will occur:
<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1 ? 0 1 ? 0 <span style="color:red;">1 0</span>
</div><br>
We will be able to recover the erasured bits.
<br><br>
</span>
<span class="fragment">
Are we done? 
</span>

</div>

---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa1.PNG){width=900px height=400px}





---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa2.PNG){width=900px height=400px}


## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa3.PNG){width=900px height=400px}

---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa4.PNG){width=900px height=400px}

---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa5.PNG){width=900px height=400px}

---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa6.PNG){width=900px height=400px}

---

## Possible Solution
### Overlapping Pairity Checks

instead of partitioning the data into distinct parts, we can use overlapping pairity checks among the data bits.

- Consider the following 4-bit example:<br>
![](assets/ldpc_exa7.PNG){width=900px height=400px}

---

## Possible Solution  
### Overlapping Parity Checks

- <span style="font-size:0.8em">In the overlapping parity check example, we showed how to correct any 2 erasures in a 4-bit message. </span>

- <span style="font-size:0.8em">What would happen if we apply this strategy repeatedly over consecutive 4-bit blocks in a longer message?</span>

![](assets/4-bit_ex.PNG){width=600px height=150px fig-align="center"}

- <span style="font-size:0.8em">And what if we increase the overlap size together with the amount of pairity checks per overlap — how does that impact performance and error correction?</span>

![](assets/16-bit_ex.PNG){width=800px height=140px fig-align="center"}

---

## Possible Solution
### Parity Check Bits Protection

<!-- TODO continue the intuition with this case -->

<div style="text-align: center; font-family: monospace; font-size: 1.5em;">
1  0  ?  1  1  0  <span class="fragment" style="color:red;">?  1  ?</span>
</div>

- Consider our previous example:
![](assets/ldpc_exa7.PNG){width=900px height=400px}

- Assume that the erasures are as follows:




- A natural question arises: how are the **parity check bits** protected, given that they are also subject to substitution errors?

- The solution lies in the **structure of LDPC codes**:  
  Parity bits are **not isolated**—they are also included in other parity check equations.

- By involving **both data and parity bits** in the parity constraints, LDPC codes ensure that **parity bits are themselves protected** and can be recovered during decoding.

- This design enables the **belief propagation algorithm** to iteratively correct errors in both data and parity bits, and effectively recover all the data.


## Density and the Role of Overlap

- Therefore, our goal is to design a parity-based error correction code that:
  - Has **low write cost**,  
  - Enables **fast and efficient decoding**,  
  - **recovers erasures effectively**.

- In the 1960s, Robert Gallager introduced a groundbreaking approach for this challenge — now known as LDPC codes (Low-Density Parity-Check codes).

---

## Density and the Role of Overlap
- Gallager showed how to **balance two competing goals**:
  - Reducing the code's redundancy (which favors larger overlapping neighborhoods),
  - With **low decoding complexity** and **High recovery power** (which favors smaller overlaps).

- He introduced the concept of **density**, which is a measure of how many bits participate in each parity check:
  - **Higher density** means more overlap (stronger recovery),
  - **Lower density** means less overlap (faster decoding).

:::{.smaller}
## Low Density

::: {.incremental}

- Gallager showed that even **sparse** parity-check matrices can achieve strong error correction, by using efficient **decoding algorithms** like belief propagation.

- Sparsity reduces complexity while still maintaining performance close to capacity.

- Gallager also showed that **randomly generated** sparse matrices usually perform well.

- Randomness avoids problematic patterns in the code structure.

- In practice, carefully designed random LDPC matrices approach the Shannon limit.

:::

:::

## LDPC Theory

- A LDPC code can be presented as a parity-check matrix $H$.

- The matrix is composed of $n$ columns and $k$ rows, where each column represents a bit (pairity bit or regular bit), and each row represents a parity-check equation over some bits.

- Each row has only a few non-zero entries, which mean that the pairity checks have low Density.

- A vector $\bar{v}\in \{0,1\}^{nX1}$ is a codeward if $Hv=\bar{0}$.

### Example

Let $n = 8$, and $k = 3$. Then one possible $H$ matrix:

$$
H =
\begin{bmatrix}
1 & 1 & 0 & 1 & 0 & 0 & 0 & 0\\
0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 & 1 & 1 & 0
\end{bmatrix}
$$

---

## Example - cont. {.smaller}

We use the same parity-check matrix:

$$
H =
\begin{bmatrix}
1 & 1 & 0 & 1 & 0 & 0 & 0 & 0\\
0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 0 & 1 & 1 & 0
\end{bmatrix}
$$

Let’s check if the vector

$$
c =
\begin{bmatrix}
1 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 1 \\ 0
\end{bmatrix}
$$

is a codeword by computing $Hc$:

---

### Computation {.smaller}

We compute:

<div style="font-size:0.6em;">

$$
Hc =
\begin{bmatrix}
1 & 1 & 0 & 1 & 0 & 0 & 0 & 0\\
0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 & 1 & 1 & 1 & 0
\end{bmatrix}
\begin{bmatrix}
1 \\ 0 \\ 1 \\ 0 \\ 0 \\ 1 \\ 1 \\ 0
\end{bmatrix}
$$

</div>

$$
\Downarrow
$$

<div style="font-size:0.8em;">
\begin{align*}
1\cdot x_1 + 0\cdot x_2 + 0\cdot x_4 &= 1 + 0 + 0 = 1 \\
0\cdot x_2 + 1\cdot x_3 + 0\cdot x_8 &= 1 + 0 + 0 = 1 \\
1\cdot x_1 + 0\cdot x_5 + 1\cdot x_6 + 1\cdot x_7 &= 1 + 0 + 1 + 1 = 1
\end{align*}


</div>

<br>
So this is **not** a codeword, because $Hc \neq 0$.

---

## LDPC Encoding

- We introduced LDPC codes using a **parity-check matrix** $H$, where a vector $\bar{v}$ is a **codeword** if it satisfies:
$$
H\bar{v} = 0.
$$

- Our goal now is to generate such codewords from raw data — in other words, to define an **encoding matrix** $G$ such that:
$$
\bar{v} = G\bar{d}
$$
for some data vector $\bar{d}$, and the result $\bar{v}$ is guaranteed to satisfy $H\bar{v} = 0$.

- Since any encoded vector $\bar{v}$ is a **linear combination of the columns of $G$**, This means that the set of all possible codewords is exactly the **column space** of $G$.
  
- Therefore, it must hold $HG=\{0\}$

---

- A common way to construct such a $G$ is to first put $H$ into **systematic form**:
$$
H = \left[ P \,\middle|\, I_m \right]
$$
where $P$ is a matrix of size $m \times k$ and $I_m$ is the identity matrix of size $m = n - k$.

- Then, the generator matrix $G$ is defined as:
$$
G = \left[ I_k \,\middle|\, P^\top \right]
$$
where $I_k$ is the identity matrix of size $k$ and $P^\top$ is the transpose of $P$.


## LDPC - Decoding Introduction

- Checking if a vector is a valid codeword via matrix multiplication with $H$ can be computationally expensive.

<br>

- However, **$H$ is sparse** in LDPC codes.

<br>

- We can exploit this sparsity by representing $H$ as a **graph structure**, enabling more efficient decoding.

<br>

- We present the concept of tanner graphs.

---

## Tanner Graphs: Definition

- A **Tanner graph** is a **bipartite graph** with:
  - **Variable nodes**: one for each bit (column of $H$)
  - **Check nodes**: one for each parity-check (row of $H$)
- An edge connects variable node $v_j$ and check node $c_i$ if and only if $H_{i,j} = 1$.

![](assets/tanner.PNG){width=900px height=400px fig-align="center"}

---

## Tanner Graphs : Example

![](assets/tanner.PNG){width=900px height=400px fig-align="center"}


<div class="fragment">
The corresponding matrix is:
$$
H =
\begin{bmatrix}
1 & 1 & 0 & 0 & 1 & 0 \\
0 & 0 & 1 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 1
\end{bmatrix}
$$

</div>

---

<!--

## Gallager’s Bit-Flipping Algorihm

- Bit-Flipping is an iterative decoding algorithm, presented by Gallagher. <br> <br>
- Bit-Flipping is used for decoding LDPC codes over a **Binary Symmetric Channel (BSC)**. <br><br>
- The core idea of the algortihm is to flip bits that participate in **many unsatisfied parity checks**.

---

## Algorithm Overview

1. **Input**: Received vector $\bar{r}$ (possibly corrupted), and parity-check matrix $H$.
2. **Check**: For each parity check, compute whether it's satisfied usign Tanner graph.
3. **Count**: For each bit, count how many unsatisfied checks it participates in.
4. **Flip**: Bits involved in "enough" unsatisfied checks are flipped iteratively.
5. **Repeat**: Iterate until all checks are satisfied or a max number of steps is reached.

---

## Example

Let:
$$
H =
\begin{bmatrix}
1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\
0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & 0 & 1 &  \,0
\end{bmatrix}
\quad \bar{r} = [0\ 1\ 0\ 0\ 1\ 0\ 0]
$$

And the Tanner graph:

![](assets/tanner2.PNG){width=800px height=270px fig-align="center"}


---

## Step 1: Compute Syndromes

We check whether the pairity checks are satisfied:
![](assets/tanner3.PNG){width=800px height=400px fig-align="center"}

Almost all checks are unsatisfied! We count how many unsatisfied checks each bit participates in.

---

## Step 2: Flip Bits

![](assets/tanner3.PNG){width=500px height=200px fig-align="center"}

<div style="font-size:0.8em;">

- Bit 1 appears in 1 unsatisfied check  
- Bit 2 appears in 2 unsatisfied checks.
- Bit 3 appears in 1 unsatisfied check. 
- Bit 4 appears in 1 unsatisfied check.
- Bit 5 appears in 3 unsatisfied checks.
- Bit 6 appears in 1 unsatisfied check.
- Bit 7 appears in 1 unsatisfied checks.

Then we flip bit 5.

</div>


---

## Step 2: Flip Bits

![](assets/tanner4.PNG){width=520px height=220px fig-align="center"}

<div style="font-size:0.8em;">
- Bit 1 appears in 1 unsatisfied check  
- Bit 2 appears in 3 unsatisfied checks.
- Bit 3 appears in 2 unsatisfied check. 
- Bit 4 appears in 1 unsatisfied check.
- Bit 5 appears in 1 unsatisfied checks.
- Bit 6 appears in 0 unsatisfied check.
- Bit 7 appears in 0 unsatisfied checks.

Then we flip bit 2.

</div>

---


## Step 2: Flip Bits

![](assets/tanner5.PNG){width=800px height=400px fig-align="center"}

This is Our Codeword!



---

## Summary

- Bit-Flipping is simple and intuitive.  
- Works well with **sparse LDPC matrices**.  
- Not optimal, but useful as a first decoding step or when low complexity is needed.

</span>

-->

## LDPC Decoding: Belief propagation algorithm

::: {.center}
# 6 Encoding/Decoding Schema {data-name="Schema"}
:::

## 3 challenges in using DNA for data storage
| **Error Type**              | **Solution**                                                               |
|----------------------------|---------------------------------|
| 1. Substitution errors      | • LDPC codes                                                                 |
| 2. Unordered reads          | • Addressing index<br>• BCH codes                                            |
| 3. Insertion/deletion errors | • Convert to substitution errors:<br> • Sync markers<br> • MSA (via indexed clusters) |

<!-- TODO(Matan): Add Index and MSA explanation -->

## Index (+ BCH)

## Multiple Sequence Alignment (MSA)

## Synchronization Marker

- **Problem:** Insertions and deletions ("indels") shift the alignment of bases, making decoding difficult.
- If a read has unexpected length (due to indels), we try to **recover the marker** using Multiple Sequence Alignment (MSA).
- If the marker is located:
  - We **split** the read at the marker.
  - Retain only the expected-length portion.
  - Mark the rest as **erasures**.
- These erasures are then passed into the LDPC decoder.
- In simulations: 10% improvement in the reading cost while having little impact (2-3%) on the writing cost.

::: {.r-vstack style="align-items:center; justify-content:center; gap:6px;"}

<!-- ─────────── 1 ►  COLOURED BAR  ─────────── -->
::: {.r-hstack style="align-items:center; justify-content:center;"}

::: {data-id="payloadL"
     style="background:#ff9f1c;  width:200px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;"}
Payload
:::

::: {data-id="sync"
     style="background:#38b000;  width:100px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;"}
AGT
:::

::: {data-id="payloadR"
     style="background:#ff9f1c;  width:200px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;"}
Payload
:::
:::
:::

## Encoding a single binary file {auto-animate=true auto-animate-easing="ease-in-out"}

- File size: $192_\text{KB} = 192 \times 1000_\text{B} = 1 536 000_\text{bits}$
- Encoding steps from: 
[https://github.com/shubhamchandak94/LDPC_DNA_storage](https://github.com/shubhamchandak94/LDPC_DNA_storage/blob/master/dna_storage.py)
- KB is decimal (1000 bytes = 1 KB), not binary (1024 bytes = 1 KiB)

::: {.r-hstack style="align-items: center; justify-content: center;"}
::: {data-id="strand1"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand2"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand3"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand4"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand5"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand6"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
:::

<!-- ------------- SLIDE 2 ------------- -->
## Large block LDPC encoding {auto-animate=true auto-animate-easing="ease-in-out"}

- LDPC data-block size (`LDPC_dim=256K`): $256 000_\text{bits}$
- Number of data blocks: $\frac{1 536 000}{256 000} = 6$
- Added parity bits (`LDPC_alpha=0.5`): For each data block, we add $128 000_\text{bits}$ of parity bits. 
- Encoded bits per block: $$256 000_\text{bits} + 128 000_\text{bits} = 384 000_\text{bits}$$

::: {.r-hstack style="align-items:center; justify-content:center;"}

<!-- container keeps the total width at exactly 700 px -->
::: {.r-hstack style="width:700px; gap:0px;"}

<!-- 6 orange data pieces -->
::: {data-id="strand1"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand2"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand3"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand4"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand5"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand6"  style="background:#ff9f1c; width:calc(700px/12); height:60px;"}
:::  

<!-- 6 blue parity pieces -->
::: {data-id="strand7"  style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand8"  style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand9"  style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand10" style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand11" style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  
::: {data-id="strand12" style="background:#1982c4; width:calc(700px/12); height:60px;"}
:::  

:::
:::

<!-- ───────────────── SLIDE 3 ───────────────── -->
## Segment and  map to DNA {auto-animate=true auto-animate-easing="ease-in-out"}

- Binary mapping: $00 \to A$, $01 \to C$, $10 \to G$, $11 \to T$
- Bits per oligo (`payload size=84bp`): $84 \times 2_\text{bits} = 168_\text{bits}$
- Number of oligos per block: $$\frac{\text{payload bits}}{\text{bits per oligo}}=\frac{384 000_\text{bits}}{168_\text{bits}} = 2285.71 \approx 2286_\text{oligos}$$
- Total number of oligos: $\text{blocks} \times \text{oligos per block} = 6 \times 2286 = 13716$ (✔)

::: {.r-hstack style="gap:60px;justify-content:center;"}

<!-- ─────────── column 1 ─────────── -->
::: {.r-vstack style="gap:10px;"}
::: {data-id="strand1" style="background:#ff9f1c;width:230px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand2" style="background:#ff9f1c;width:230px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand3" style="background:#ff9f1c;width:230px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand4" style="background:#ff9f1c;width:230px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand5" style="background:#ff9f1c;width:230px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand6" style="background:#ff9f1c;width:230px;height:24px;border-radius:4px;"}
:::
:::

<!-- ─────────── column 2 ─────────── -->
::: {.r-vstack style="gap:10px;"}
::: {data-id="strand7"  style="background:#1982c4;width:165px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand8"  style="background:#1982c4;width:165px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand9"  style="background:#1982c4;width:165px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand10" style="background:#1982c4;width:165px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand11" style="background:#1982c4;width:165px;height:24px;border-radius:4px;"}
:::
::: {data-id="strand12" style="background:#1982c4;width:165px;height:24px;border-radius:4px;"}
:::
:::
:::


## Encoded oligo structure (/W Sync markers) {auto-animate=true auto-animate-easing="ease-in-out"}
| Segment                             | bp         |
| ----------------------------------- | ---------- |
| BCH-protected index                 | $14_\text{bit} / 2 = 7_\text{bp}$      |
| BCH redundancy      | $12_\text{bit} / 2 = 6_\text{bp}$       |
| Sync marker (`AGT`)            | $3_\text{bp}$       |
| **Payload** | $84_\text{bp}$      |
| Primers                   | $2 \times 25_\text{bp}$       |
| **Total**                            | **$150_\text{bp}$**  |

::: {.r-vstack style="align-items:center; justify-content:center; gap:6px;"}

<!-- ─────────── 1 ►  COLOURED BAR  ─────────── -->
::: {.r-hstack style="align-items:center; justify-content:center;"}

::: {data-id="primerL"
     style="background:#6c757d;  width:100px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;
            border-top-left-radius:6px; border-bottom-left-radius:6px;"}
FW
:::

::: {data-id="indexECC"
     style="background:#d90429;  width:200px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;"}
Index + ECC
:::

::: {data-id="payloadL"
     style="background:linear-gradient(135deg,
                   #ff9f1c 0%,   /* orange in the upper-left corner   */
                   #1982c4 100%);/* blue in the lower-right corner    */
       width:200px; height:60px;
       color:#fff; font-weight:bold;
       display:flex; align-items:center; justify-content:center;"
}
Payload
:::

::: {data-id="sync"
     style="background:#38b000;  width:100px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;"}
AGT
:::

::: {data-id="payloadR"
     style="background:linear-gradient(135deg,
                   #ff9f1c 0%,   /* orange in the upper-left corner   */
                   #1982c4 100%);/* blue in the lower-right corner    */
       width:200px; height:60px;
       color:#fff; font-weight:bold;
       display:flex; align-items:center; justify-content:center;"
}
Payload
:::

::: {data-id="primerR"
     style="background:#6c757d;  width:100px; height:60px;
            color:#fff; font-weight:bold;
            display:flex; align-items:center; justify-content:center;
            border-top-right-radius:6px; border-bottom-right-radius:6px;"}
REV
:::
:::

<!-- ─────────── 2 ►  bp LABEL ROW  ─────────── -->
::: {.r-hstack style="gap:0px; font-size:0.8em; font-weight:bold; color:#000;"}

::: {data-id="primerL_lbl"   style="width:100px; text-align:center;"}
25 bp
:::

::: {data-id="indexECC_lbl"  style="width:200px; text-align:center;"}
6+7 bp
:::

::: {data-id="payloadL_lbl"  style="width:200px; text-align:center;"}
42 bp
:::

::: {data-id="sync_lbl"      style="width:100px; text-align:center;"}
3 bp
:::

::: {data-id="payloadR_lbl"  style="width:200px; text-align:center;"}
42 bp
:::

::: {data-id="primerR_lbl"   style="width:100px; text-align:center;"}
25 bp
:::
:::
:::

## Encoding schema overview

::: {.r-stack style="height:80vh; display:flex; flex-direction:column; justify-content:center; align-items:center;"}
![](assets/encoding.png)
:::

## Decoding schema overview
<!-- TODO(Mattan) -->


::: {.center}
# 7 Experimental results {data-name="Results"}
:::

## Write/read costs (Fig. 8 / Table 1) {.smaller}

<!-- Table block -->
<div style="position:relative;">

| Exp. No. | LDPC Redundancy | File Size | No. of Oligonucleotides | Normalized Coverage Variance | Writing Cost (bases/bit) | Reading Cost (bases/bit) |
|----------|------------------|-----------|--------------------------|-------------------------------|---------------------------|---------------------------|
| 1        | 50%              | 160 KB    | 11,710                   | 1.97                          | 0.91                      | 2.73                      |
| 2        | 10%              | 224 KB    | 12,026                   | 1.57                          | 0.67                      | 3.82                      |
| 3        | 50%              | 192 KB    | 13,716                   | 3.36                          | 0.89                      | 3.45                      |
| 4        | 30%              | 192 KB    | 11,892                   | 3.53                          | 0.78                      | 4.46                      |
| 5        | 10%              | 192 KB    | 10,062                   | 3.19                          | 0.66                      | 8.11                      |

<!-- Floating figure -->
<div class="fragment"
     style="position:absolute; top:50px; right:-60px; width:790px; z-index:10;">
  <img src="assets/figure8.png" style="width:150%;">
</div>

</div>


## Error profile (Fig. 9)
::: {.r-stack style="height:80vh; display:flex; flex-direction:column; justify-content:center; align-items:center;"}
![](assets/figure9.png)
:::

## Coverage profile (Fig. 10)
::: {.r-stack style="height:80vh; display:flex; flex-direction:column; justify-content:center; align-items:center;"}
![](assets/figure10.png)
:::


## Indel correction heuristics (Table. 4)
<!-- | LDPC Redundancy | Writing Cost | Reading Cost (1 Indel) | Reading Cost (0 Indels) |
|------------------|---------------|--------------------------|--------------------------|
| 50%              | 0.89          | 3.45                     | 3.58                     |
| 30%              | 0.78          | 4.46                     | 4.73                     |
| 10%              | 0.66          | 8.11                     | 8.57                     | -->

<div style="center; position:relative;">
![](assets/table4.png){fig-align="center" width="80%"}
</div>

- Attempting to correct single indel with a BCH code
- This step is able to correct 5-10% additional indexes.

## Probability of dec. failure (Fig. 11)
::: {.r-stack style="height:80vh; display:flex; flex-direction:column; justify-content:center; align-items:center;"}
![](assets/figure11.png)
:::

<!-- TODO(Matan): Add a figure with the probability of decoding failure -->
## Stress testing

* Simulated **6% total error** (2% sub / 2% del / 2% ins)
* Added **15% random reads** (unaligned noise)
* **224 KB** file, **50% LDPC**, BCH (3-error correction)
* Increased LDPC decoder threshold to **10%**
* **Write cost**: 1.07 bases/bit
* **Decoding succeeded** at **10.5 bases/bit read cost**


::: {.center}
# 8 Concluding Remarks {data-name="Conclusions"}
:::

## Conclusions

* Achieves **better read/write cost tradeoff** than prior work
* Combines **LDPC codes** with **heuristics** for indel correction
* Insights may help improve **bioinformatics tools** and **error models**

## Future Work

* Use **channel-optimized LDPC** or **marker codes**
* Extend to **nanopore sequencing** (high indel rates)
* Improve **index error correction efficiency**

## Limitations
* **Random access** No range queries or selective decoding
* **Counting on heuristics** for indel correction, which may not generalize well
* **Error model** Assumes independent errors, which may not hold in practice
* **Relying on simulations** for performance evaluation, which may not fully capture real-world complexities

<!-- TODO(Atar & Matan): Think of improvments -->



## Crafting the Presentation: Tools
- [`Quarto`](https://quarto.org/): markdown-based authoring system that supports multiple output formats.
- [`revealjs`](https://revealjs.com/): a framework for creating interactive presentations using HTML and JavaScript.
  - [`simplemenu`](https://github.com/Martinomagnifico/quarto-simplemenu): a plugin to create a menu bar that allows us to navigate through the presentation.

## Ideas

- Gallager youtube talk at the end of the slides
- Example of LDPC using [pyldpc](https://hichamjanati.github.io/pyldpc/auto_examples/plot_coding_decoding_simulation.html#sphx-glr-auto-examples-plot-coding-decoding-simulation-py)